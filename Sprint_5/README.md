# üöÄ Sprint 5

## üìå Resumo

Durante a Sprint 5, aprofundei meus conhecimentos em Apache Spark al√©m de alguns conceitos da AWS, por meio de cursos, exerc√≠cios e do desafio proposto. Foi meu primeiro contato com o Spark, e essa experi√™ncia me permitiu consolidar o aprendizado de maneira pr√°tica, din√¢mica e colaborativa. As discuss√µes frequentes com os colegas da squad foram essenciais para superar dificuldades e refor√ßar o entendimento dos conte√∫dos, destaco tamb√©m a monitoria, que proporcionou trocas enriquecedoras e debates interessantes. A seguir, apresento um resumo dos principais t√≥picos abordados:

- **Data & Analytics**: atrav√©s do D&A, pude ter uma maior base te√≥rica sobre o Apache Hadoop e o Apache Spark, compreendendo seus principais componentes, arquiteturas e casos de uso. Estudei como o Hadoop permite o processamento distribu√≠do de grandes volumes de dados por meio do HDFS e do MapReduce, bem como a forma como o Spark se destaca por sua velocidade e capacidade de processamento em mem√≥ria. Al√©m disso, tamb√©m forneceu as principais informa√ß√µes para seguir com a realiza√ß√£o dos exerc√≠cios e do desafio.

- **Forma√ß√£o Spark com Pyspark**: aprendi a configurar o Spark, manipular DataFrames, aplicar transforma√ß√µes e a√ß√µes, usar SQL, views e joins no Pyspark. Trabalhei com dados em disco criando tabelas em formatos Parquet e ORC, desenvolvimento de aplica√ß√µes execut√°veis via linha de comando, tamb√©m explorei t√©cnicas de otimiza√ß√£o e integrei o Spark com Jupyter e bibliotecas Python.

- **Fundamentals of Analytics on AWS ‚Äì Part 1 (Portugu√™s)**: fui apresentada aos conceitos fundamentais de analytics, incluindo os tipos de an√°lise de dados, os 5 Vs do big data e os desafios associados ao processamento de grandes volumes de dados. O treinamento tamb√©m abordou como os servi√ßos da AWS se alinham a cada um desses aspectos, oferecendo solu√ß√µes abrangentes para armazenamento, transporte, processamento e an√°lise de dados. Al√©m disso, tamb√©m explorei conceitos de machine learning na AWS juntamente com uma breve vis√£o geral desse assunto.

- **Introduction to Amazon Athena (Portugu√™s)**: nesse curso aprendi um pouco mais sobre o Amazon Athena, servi√ßo serverless de consultas SQL para an√°lise de dados diretamente no Amazon S3. Tamb√©m s√£o discutidas as etapas b√°sicas da implementa√ß√£o do mesmo, al√©m de ser realizada uma breve demonstra√ß√£o da cria√ß√£o de um banco de dados para executar consultas SQL para valida√ß√£o.

- **Serverless Analytics (Portugu√™s)**: por meio desse curso aprofundei sobre as solu√ß√µes de an√°lise de dados serverless na AWS, explorando servi√ßos como AWS IoT Analytics, Amazon Cognito, AWS Lambda e Amazon SageMaker. Aprendi a coletar, processar e disponibilizar dados de forma escal√°vel e sem a necessidade de gerenciar servidores, facilitando decis√µes orientadas por dados em tempo real.

ü§î *Reflex√µes*

A Sprint 5 foi bastante desafiadora para mim, j√° que ainda estou nos meus primeiros contatos com a AWS. Por√©m, agora j√° sinto que estou me adaptando melhor, as dificuldades que enfrentei consegui super√°-las com o apoio do meu squad. √Ä medida que fui avan√ßando nos exerc√≠cios, fui ganhando mais confian√ßa, e concluir o desafio final foi bastante gratificante. Al√©m disso, me sinto muito grata por todo o suporte que venho recebendo dos monitores, da Scrum Master, do meu time e dos demais membros da PB.


<br>

---

## üóÇÔ∏è Sum√°rio 

1. [Desafio](#desafio)

2. [Exerc√≠cios](#exerc√≠cios)
    - 2.1 [Apache Spark - Contador de palavras](#21-apache-spark---contador-de-palavras)
    - 2.2 [Exerc√≠cio TMDB](#22-exerc√≠cio-tmdb)

3. [Certificados](#certificados)

<br>

---

# [Desafio](./Desafio/) 

Neste desafio da Sprint 5, o objetivo foi construir a parte inicial do Data Lake na AWS, aplicando na pr√°tica os conceitos de ingest√£o, armazenamento e organiza√ß√£o de dados em nuvem. O tema escolhido para a an√°lise foi a evolu√ß√£o da representa√ß√£o feminina em filmes de guerra ao longo das d√©cadas, buscando entender tanto a participa√ß√£o de mulheres nos elencos, quanto a presen√ßa de diretoras e poss√≠veis varia√ß√µes por regi√£o geogr√°fica.

A primeira etapa do desafio foi a defini√ß√£o dos questionamentos, baseados em uma an√°lise explorat√≥ria inicial do arquivo movies.csv e nas possibilidades de enriquecimento dos dados via API do TMDB. Essa defini√ß√£o foi fundamental para direcionar todas as demais etapas de ingest√£o e processamento de dados. Na sequ√™ncia, realizei a ingest√£o batch dos dados locais, utilizando Python e a biblioteca boto3 para o envio dos arquivos CSV ao Amazon S3, organizando-os na camada RAW, seguindo o padr√£o de particionamento por tipo de dado e data de processamento.

Na segunda etapa, desenvolvi um processo de ingest√£o via API, com foco na coleta de informa√ß√µes complementares aos CSVs, utilizando uma fun√ß√£o AWS Lambda. Os arquivos utilizados para a realiza√ß√£o do desafio est√£o organizados em pastas por etapas, acompanhando as fases do desenvolvimento, e podem ser encontrados na *Pasta Desafio*. As evid√™ncias do processo est√£o armazenadas na *Pasta Evid√™ncias*. Para um detalhamento completo do desafio, recomendo consultar o README da pasta *Readme Desafio*. Seguem os links:

- [Pasta Desafio](./Desafio/) 
- [Pasta Evid√™ncias](./Evid√™ncias/)
- [Readme Desafio](./Desafio/README.md)


<br>

---

# [Exerc√≠cios](./Exerc√≠cios/)

## 2.1 Apache Spark - Contador de palavras

Neste desafio, a proposta foi de realizar uma atividade utilizando o framework Apache Spark dentro de um container Docker. A atividade envolveu desde o download da imagem jupyter/all-spark-notebook, que j√° vem com o Spark e o Jupyter instalados, at√© a cria√ß√£o de um container, o acesso ao Jupyter Lab e a utiliza√ß√£o do shell PySpark. O arquivo analisado foi o meu *README Principal* do reposit√≥rio do GitHub, o qual foi baixado e processado utilizando comandos do Spark. Mais adiante, vou detalhar os passos que segui, os comandos utilizados e os resultados obtidos durante a execu√ß√£o dessa tarefa.

| Arquivo | Link |
|--------|------|
| Exerc√≠cio1.py | [üîó Exerc√≠cio1.py](./Exerc√≠cios/Exerc√≠cio1/Exercicio1.py) |
| Readme.md | [üîó Readme.md](../README.md) |
<br>

>Resolu√ß√£o:

**Contador de palavras**

Etapa 1 ‚Äì Comecei a primeira etapa acessando o terminal da minha m√°quina e utilizando o comando docker pull para baixar a imagem jupyter/all-spark-notebook. O processo de download ocorreu normalmente, embora tenha levado alguns minutos devido ao tamanho da imagem.

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia1.png)

Etapa 2 ‚Äì Na segunda etapa, criei um container a partir da imagem jupyter/all-spark-notebook utilizando o comando docker run. Para garantir o acesso ao Jupyter Lab, executei o container no modo interativo e realizei o mapeamento da porta padr√£o 8888 do container para a minha m√°quina local. Durante a inicializa√ß√£o, diversos logs foram exibidos no terminal, incluindo o link de acesso ao Jupyter. Copiei a URL gerada e colei essa URL no navegador onde consegui acessar o ambiente do Jupyter Lab com sucesso, o que confirmou que o container estava funcionando corretamente e o ambiente estava pronto para os pr√≥ximos passos.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia2.png)

Etapa 3 ‚Äì Na terceira etapa, dentro do ambiente do Jupyter, utilizei o comando !wget para baixar o arquivo principal README.md do meu reposit√≥rio no GitHub, utilizando o link direto da vers√£o bruta, o raw. Para garantir que o arquivo fosse salvo com o nome correto, adicionei o par√¢metro -O README.md ao final do comando, assim, download foi realizado com sucesso e o arquivo ficou dispon√≠vel no ambiente do Jupyter para ser utilizado nas pr√≥ximas etapas de processamento com o Spark.

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia3.png)

Ainda no jupyter testei o c√≥digo que criei, onde importei as bibliotecas necess√°rias e realizei a leitura do arquivo README.md utilizando o m√©todo spark. Para preparar o texto para a contagem de palavras, apliquei a fun√ß√£o regexp_replace para substituir todos os caracteres que n√£o fossem letras, espa√ßos ou tra√ßos por espa√ßos, garantindo que emojis e s√≠mbolos indesejados fossem removidos sem unir palavras. Depois, substitu√≠ os tra√ßos e travess√µes restantes por espa√ßos para separar corretamente palavras que estivessem conectadas por esses sinais. Utilizei a fun√ß√£o split para dividir cada linha em uma lista de palavras, separando por espa√ßos, e explode para transformar essa lista em m√∫ltiplas linhas, uma para cada palavra. Em seguida, normalizei as palavras convertendo todas para min√∫sculas e removendo espa√ßos em branco extras com lower e trim. Filtrei as palavras vazias para evitar registros inv√°lidos na contagem. Por fim, agrupei as palavras e contei suas ocorr√™ncias usando groupBye count, ordenando o resultado de forma decrescente para listar as palavras mais frequentes primeiro. O resultado foi exibido com show, apresentando a contagem de todas as palavras contidas no arquivo.

| Arquivo | Link |
|--------|------|
| Exerc√≠cio1.py | [üîó Exerc√≠cio1.py](./Exerc√≠cios/Exerc√≠cio1/Exercicio1.py) |
<br>

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia4.png)

Ap√≥s validar o c√≥digo no Jupyter, pude ent√£o abrir o terminal diretamente, via PySpark interativo. Para isso, executei docker ps para encontrar o ID do meu container ativo e, em seguida, usei docker exec -it edd56b83f0d6 pyspark para acessar o shell interativo.

![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia5.png)

Etapa 4 ‚Äì Dentro do PySpark, executei linha a linha o c√≥digo previamente prototipado. O tratamento do texto foi conclu√≠do com sucesso e, ao final, o terminal exibiu a contagem das palavras mais recorrentes do arquivo README.md.

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia6.png)

<br>

## 2.2 Exerc√≠cio TMDB

Neste desafio, a proposta foi realizar uma atividade utilizando a API p√∫blica do The Movie Database com o objetivo de extrair dados sobre filmes. Assim, foi necess√°rio realizar a cria√ß√£o de uma conta no portal, solicita√ß√£o de chave de acesso e entendimento dos endpoints da API, que fornece dados sobre filmes, s√©ries, g√™neros e classifica√ß√µes.

Por meio desse exerc√≠cio tamb√©m foi poss√≠vel compreender a estrutura da API RESTful do TMDB e preparar o ambiente para extra√ß√£o de dados com Python. A seguir, vou detalhar os passos que segui, os comandos utilizados e os resultados obtidos durante a execu√ß√£o dessa tarefa.

| Arquivo | Link |
|--------|------|
| Exerc√≠cio2.ipynb| [üîó Exerc√≠cio2.ipynb](./Exerc√≠cios/Exerc√≠cio2/Exercicio2.ipynb) |
<br>

>Resolu√ß√£o:

**Exerc√≠cio TMDB**

Etapa 1 ‚Äì Criei minha conta no site do TMDB, confirmei o e-mail e acessei meu perfil para solicitar a chave da API. No formul√°rio, selecionei o tipo de uso como Pessoal, informei o nome "TMDB API - Projeto Educacional", usei o link fict√≠cio https://meuapp-tmdb.com e descrevi que o uso seria para fins educacionais. Ao final, recebi minha chave de API e finalizei a etapa com sucesso.

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia1.png)

Etapa 2 ‚Äì Iniciei a segunda etapa configurando o ambiente para testar a API do TMDB. Criei um arquivo .env para armazenar minhas credenciais de forma segura, a chave da API e o token de acesso. Tamb√©m configurei o .gitignore para evitar que essas informa√ß√µes sens√≠veis fossem enviadas ao reposit√≥rio no GitHub. Ap√≥s isso, testei o c√≥digo criado para consumir a API do TMDB e exibir os filmes mais bem avaliados. Iniciei importei as bibliotecas necess√°rias: requests para realizar a requisi√ß√£o HTTP, pandas para estruturar os dados em formato tabular, dotenv para carregar as vari√°veis de ambiente do arquivo .env, e os para acessar essas vari√°veis dentro do script.

Configurei a leitura do arquivo .env usando o m√©todo load_dotenv, garantindo que a chave da API fosse acessada com seguran√ßa, sem ser exposta diretamente no c√≥digo. Em seguida, montei o endpoint da API  com os par√¢metros necess√°rios, como a chave e o idioma da resposta.

Fiz a requisi√ß√£o usando requests.get e, ao confirmar que o status da resposta era 200, extra√≠ os dados do JSON retornado. Percorri a lista de filmes com um loop, selecionando as principais informa√ß√µes: t√≠tulo, nota m√©dia e data de lan√ßamento. Com esses dados, montei uma lista de dicion√°rios que foi convertida em um DataFrame do pandas, permitindo uma visualiza√ß√£o estruturada dos filmes. Por fim, usei display(df) para exibir os dados em tabela no notebook. Com isso, pude validar o funcionamento da API e a integra√ß√£o das credenciais, exibindo com sucesso os t√≠tulos, notas e datas de lan√ßamento dos filmes mais bem avaliados. 

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia2.png)


<br>

---


# Certificados


Durante essa sprint, conclu√≠ os cursos AWS: Fundamentals of Analytics on AWS ‚Äì Part 1 (Portugu√™s), Introduction to Amazon Athena (Portugu√™s) e Serverless Analytics (Portugu√™s). A seguir, compartilho os certificados correspondentes:

| Certificado | Link |
|--------|------|
|Fundamentals of Analytics on AWS | [üîó Fundamentals of Analytics on AWS](./Certificados/Fundamentals%20of%20Analytics%20on%20AWS%20‚Äì%20Part%201%20(Portugu√™s).pdf) |
|Introduction to Amazon Athena | [üîó Introduction to Amazon Athena](./Certificados/Introduction%20to%20Amazon%20Athena%20(Portugu√™s).pdf) |
|Serverless Analytics | [üîó Serverless Analytics](./Certificados/Serverless%20Analytics%20(Portugu√™s).pdf) |