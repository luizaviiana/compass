# üöÄ Desafio 

## üìå Resumo

Neste desafio da Sprint 5, o objetivo foi construir a parte inicial do Data Lake na AWS, aplicando na pr√°tica os conceitos de ingest√£o, armazenamento e organiza√ß√£o de dados em nuvem. O tema escolhido para a an√°lise foi a evolu√ß√£o da representa√ß√£o feminina em filmes de guerra ao longo das d√©cadas, buscando entender tanto a participa√ß√£o de mulheres nos elencos, quanto a presen√ßa de diretoras e poss√≠veis varia√ß√µes por regi√£o geogr√°fica.

A primeira etapa do desafio foi a defini√ß√£o dos questionamentos, baseados em uma an√°lise explorat√≥ria inicial do arquivo movies.csv e nas possibilidades de enriquecimento dos dados via API do TMDB. Essa defini√ß√£o foi fundamental para direcionar todas as demais etapas de ingest√£o e processamento de dados. Na sequ√™ncia, realizei a ingest√£o batch dos dados locais, utilizando Python e a biblioteca boto3 para o envio dos arquivos CSV ao Amazon S3, organizando-os na camada RAW, seguindo o padr√£o de particionamento por tipo de dado e data de processamento.

Na segunda etapa, desenvolvi um processo de ingest√£o via API, com foco na coleta de informa√ß√µes complementares aos CSVs, utilizando uma fun√ß√£o AWS Lambda. O desenvolvimento foi organizado em etapas, desde a prototipa√ß√£o inicial at√© a execu√ß√£o final, detalhadas nas se√ß√µes a seguir, contendo os arquivos e as evid√™ncias de cada etapa:



## üóÇÔ∏è Sum√°rio 

1. [Etapa 1](#etapa-1)
2. [Etapa 2](#etapa-2)


<br>

---

# Etapa 1


Nesta etapa, realizei a leitura e an√°lise explorat√≥ria dos arquivos movies.csv e series.csv, fornecidos pelo desafio, utilizando notebooks em Jupyter. Essas an√°lises permitiram entender a estrutura dos dados e orientar a defini√ß√£o da linha de investiga√ß√£o do projeto, focada na evolu√ß√£o da representatividade feminina em filmes de guerra a partir de 1950. Tamb√©m explorei dados da API do TMDB em um notebook separado, com o objetivo de identificar quais campos seriam √∫teis para complementar as informa√ß√µes dos arquivos CSV nas pr√≥ximas etapas.

Ap√≥s essa fase explorat√≥ria, preparei e testei o envio dos arquivos para o bucket S3 chamado data-compass-ana. Primeiro, fiz testes locais de upload usando o boto3 diretamente em um notebook, garantindo que as credenciais e a estrutura estavam corretas. Em seguida, organizei os arquivos necess√°rios para ingest√£o dentro da pasta Etapa1, que cont√©m o script upload_s3.py, o Dockerfile, o arquivo .env com as credenciais da AWS (inclu√≠do no .gitignore) e o .dockerignore para ignorar arquivos desnecess√°rios no container. A pasta data, que foi utilizada inicialmente, concentra os arquivos movies.csv e series.csv, os dois notebooks com as an√°lises explorat√≥rias: um dedicado ao CSV e outro √† API. Al√©m do notebook que foi utilizado como teste para o envio dos arquivos.



| Pasta | Link |
|--------|------|
| data | [üîó data ](./data/) |
| Etapa1 | [üîó Etapa1 ](./Etapa1/) |
<br>

> Explica√ß√£o An√°lise Inicial


**An√°lise inicial**

Como mencionado anteriormente, iniciei essa parte realizando a leitura e an√°lise dos conjuntos de dados: o arquivo movies.csv e o CSV gerado a partir da extra√ß√£o de dados da API do TMDB. Para organizar melhor o processo, optei por criar dois notebooks separados no Jupyter, essa separa√ß√£o me permitiu comparar as duas bases e entender de forma mais clara como elas poderiam se complementar nas pr√≥ximas etapas. Durante essa an√°lise explorat√≥ria, fui tamb√©m avaliando se os dados dispon√≠veis seriam suficientes para responder aos questionamentos que eu tinha interesse em investigar.

| Arquivo | Link |
|--------|------|
| MoviesCSV.ipynb | [üîó MoviesCSV.ipynb ](./data/MoviesCSV.ipynb) |
| MoviesAPI.ipynb | [üîó MoviesAPI.ipynb ](./data/MoviesAPI.ipynb) |
<br>

A partir dessas an√°lises iniciais, optei por seguir uma linha de investiga√ß√£o voltada para a evolu√ß√£o da representa√ß√£o feminina em filmes de guerra ao longo das d√©cadas. A escolha desse tema foi motivada pelo interesse em explorar como a participa√ß√£o das mulheres tem evolu√≠do nesse contexto, considerando tanto aspectos quantitativos (como a propor√ß√£o de mulheres nos elencos e a presen√ßa de diretoras) quanto qualitativos (como o perfil dos pap√©is desempenhados pelas personagens femininas e poss√≠veis varia√ß√µes regionais na produ√ß√£o desses filmes).

Al√©m disso, busquei tamb√©m comparar a representatividade feminina em rela√ß√£o √† masculina ao longo das d√©cadas, entendendo como esse cen√°rio tem se transformado com o tempo. Para garantir uma base de an√°lise mais consistente e focada em filmes com maior relev√¢ncia dentro do g√™nero, apliquei alguns filtros iniciais, considerando apenas filmes de guerra lan√ßados a partir de 1950, com nota m√©dia maior ou igual a 6 e com pelo menos 100 votos registrados no TMDB, o que ajudou a garantir uma amostra com dados mais representativos e de melhor qualidade para an√°lise.

<br>

**Questionamentos**

Por meio da an√°lise inicial, foi poss√≠vel identificar as principais informa√ß√µes e dados presentes no arquivo CSV e da API. Com base nisso, selecionei oito questionamentos principais:


 Qual a m√©dia da propor√ß√£o de mulheres nos elencos de filmes de guerra em cada d√©cada?

Objetivo: analisar a evolu√ß√£o hist√≥rica da participa√ß√£o feminina nos elencos ao longo do tempo.

- Como evoluiu a propor√ß√£o entre homens e mulheres nos elencos principais desses filmes?

Objetivo: investigar se houve mudan√ßas no equil√≠brio de g√™nero entre os pap√©is principais.

- Quais filmes de guerra tiveram o maior e o menor percentual de mulheres no elenco?

Objetivo: destacar os filmes de guerra que apresentaram os maiores e menores percentuais de mulheres no elenco, evidenciando casos de maior inclus√£o ou de baixa representatividade feminina.

- H√° diferen√ßa na recep√ß√£o cr√≠tica (nota m√©dia e n√∫mero de votos) entre filmes com maior presen√ßa feminina e os com menor presen√ßa feminina?

Objetivo: avaliar se existe alguma correla√ß√£o entre a representatividade feminina e a avalia√ß√£o do p√∫blico.

- Qual o perfil das personagens femininas nesses filmes?

Objetivo: investigar se as mulheres ocupam pap√©is de destaque (protagonistas, oficiais, guerreiras) ou se predominam em pap√©is secund√°rios.

- Como a participa√ß√£o feminina varia entre as diferentes d√©cadas e qual a rela√ß√£o com a presen√ßa de diretoras ou cineastas mulheres?

objetivo: relacionar a evolu√ß√£o da representatividade feminina no elenco com a atua√ß√£o de diretoras.

- Quais diretores est√£o associadas a uma maior representatividade feminina nos filmes de guerra?

Objetivo: mapear quais cineastas contribu√≠ram para ampliar a diversidade de g√™nero nesse g√™nero cinematogr√°fico.

- Existe alguma tend√™ncia regional ou pa√≠s onde a presen√ßa feminina nos filmes de guerra √© maior?

Objetivo: analisar se determinadas regi√µes ou pa√≠ses apresentam maior inclus√£o de mulheres em seus elencos de filmes de guerra.-


<br>

> Explica√ß√£o Etapa 1

**C√≥digo para upload**

Para realizar o upload dos arquivos locais para o bucket S3, escrevi um script em Python chamado upload_s3.py. Esse script utiliza as bibliotecas boto3 e dotenv para acessar as credenciais da AWS a partir de um arquivo .env, garantindo seguran√ßa e flexibilidade no gerenciamento de vari√°veis sens√≠veis. Estruturei o caminho de destino no S3 seguindo o padr√£o Raw/Local/CSV/{tipo}/{ano}/{m√™s}/{dia}/, permitindo uma organiza√ß√£o eficiente dos dados no Data Lake. Implementei uma fun√ß√£o chamada upload_file_to_s3, respons√°vel por fazer o upload dos arquivos movies.csv e series.csv de forma automatizada. Antes de integrar esse processo ao Docker, testei o script localmente em um notebook Jupyter, validando a autentica√ß√£o, a estrutura dos caminhos e a comunica√ß√£o com o bucket data-compass-ana.

| Arquivo | Link |
|--------|------|
| upload_s3.ipynb | [üîó upload_s3.ipynb ](./data/upload_s3.ipynb) |
| upload_s3.py | [üîó upload_s3.py ](./Etapa1/upload_s3.py) |
<br>

**Constru√ß√£o da imagem Docker**

Ap√≥s validar o funcionamento do script, preparei a estrutura necess√°ria para execut√°-lo em um ambiente containerizado com Docker. Para isso, criei um Dockerfile baseado em python:3.10-slim, defini o diret√≥rio de trabalho como /app, copiei o script para dentro da imagem e instalei as depend√™ncias listadas. Tamb√©m criei um .dockerignore para evitar que arquivos desnecess√°rios fossem copiados para a imagem, como os arquivos de dados locais, e inclu√≠ o arquivo .env no .gitignore para evitar o versionamento de informa√ß√µes sens√≠veis. Com tudo pronto, utilizei o comando docker build -t upload-s3 . para construir a imagem.

![Evid√™ncia 1](../Evid√™ncias/evidencia1.png)

Em seguida, executei o container com docker run --rm -v "${PWD}:/app" --env-file .env upload-s3.

![Evid√™ncia 2](../Evid√™ncias/evidencia2.png)

O processo foi bem-sucedido, e os arquivos foram enviados para o S3 conforme o caminho definido no script, concluindo assim a ingest√£o inicial dos dados fornecidos no desafio.

![Evid√™ncia 3](../Evid√™ncias/evidencia3.png)
<br>

![Evid√™ncia 4](../Evid√™ncias/evidencia4.png)

<br>

---

# Etapa 2

Nesta etapa, desenvolvi a ingest√£o dos dados complementares da API do TMDB, com o objetivo de enriquecer a base de filmes de guerra. A ideia foi buscar, para cada filme, informa√ß√µes adicionais ao CSV movies que s√£o fundamentais para responder √†s perguntas anal√≠ticas propostas, como estou considerando apenas filmes de guerra lan√ßados a partir de 1950, com nota m√©dia maior ou igual a 6 e com pelo menos 100 votos registrados no TMDB, apliquei esses filtros diretamente nas chamadas da API, otimizando o processo de coleta.

Para organizar essa etapa, criei tr√™s pastas principais. A primeira foi a lambda_function_test, onde concentrei os testes locais da ingest√£o. Nessa fase, escrevi dois scripts principais: o tmdb_client_test.py, respons√°vel por realizar as chamadas √† API do TMDB e retornar tanto os IDs dos filmes quanto os detalhes de cada um; e o lambda_function_test.py, que orquestrou a execu√ß√£o completa, desde a coleta dos dados at√© o envio ao S3. Com esses scripts, validei a l√≥gica de filtragem, a extra√ß√£o dos campos desejados da API, o agrupamento dos dados em arquivos .json com at√© 100 registros e o envio automatizado para o bucket S3, seguindo a estrutura de pastas definida.

A segunda pasta criada foi a lambda_layer_docker, utilizada para gerar uma camada personalizada com a biblioteca requests, j√° que essa depend√™ncia n√£o est√° dispon√≠vel por padr√£o nas fun√ß√µes AWS Lambda. Dentro dessa pasta, configurei um ambiente Docker a partir de um Dockerfile, criei um requirements.txt com a depend√™ncia necess√°ria e gerei o arquivo requests-layer.zip usando os comandos apropriados no container. Esse arquivo compactado foi posteriormente carregado como uma Lambda Layer no console da AWS e utilizado na configura√ß√£o final da fun√ß√£o.

Com os testes validados e a layer pronta, criei a terceira pasta chamada lambda_function, na qual adaptei os arquivos para o formato final esperado pela AWS Lambda. Renomeei lambda_function_test.py para lambda_function.py, removi instru√ß√µes de teste e mantive a l√≥gica automatizada e reutiliz√°vel. Tamb√©m mantive o m√≥dulo tmdb_client.py como auxiliar, garantindo a separa√ß√£o entre a coleta dos dados e a l√≥gica de orquestra√ß√£o da fun√ß√£o. Ap√≥s essas adapta√ß√µes, compactei os arquivos em um pacote .zip chamado tmdb_lambda_package.zip e fiz o upload manual no console da AWS.

A seguir estarei compartilhando o link das pastas e o detalhamento do que fiz em cada etapa:

| Pasta | Link |
|--------|------|
| lambda_function_test | [üîó lambda_function_test ](./Etapa2/lambda_function_test/) |
| lambda_layer_docker | [üîó lambda_layer_docker ](./Etapa2/lambda_layer_docker/) |
| lambda_function | [üîó lambda_function ](./Etapa2/lambda_function/) |
<br>



> Explica√ß√£o Etapa 2

**Lambda_function_test**

Essa pasta foi criada para que fosse poss√≠vel realizar os testes nos c√≥digos e valid√°-los. Assim, primeiramente, defini os crit√©rios de coleta dos filmes a partir da API do TMDB, como mencionado anteriormente, escolhi buscar apenas filmes de guerra lan√ßados a partir de 1950, com nota m√©dia maior ou igual a 6 e no m√≠nimo 100 votos. Para isso, escrevi uma fun√ß√£o buscar_ids_filmes_guerra() que realiza chamadas paginadas ao endpoint /discover/movie da API do TMDB, utilizando os par√¢metros adequados e respeitando a limita√ß√£o de requisi√ß√µes com uma pequena pausa (time.sleep(0.25)) entre os requests. Essa fun√ß√£o retorna uma lista de IDs de filmes que atendem aos crit√©rios. Em seguida, desenvolvi a fun√ß√£o para buscar os detalhes completos dos filmes, com os IDs coletados, implementei a fun√ß√£o buscar_detalhes_filme(id_tmdb), que acessa o endpoint /movie/{movie_id} com o par√¢metro append_to_response=credits,keywords,release_dates. Com isso, consegui enriquecer os dados com informa√ß√µes adicionais. Essa fun√ß√£o faz a chamada individual para cada ID retornado da busca anterior.

Com as fun√ß√µes de extra√ß√£o prontas, escrevi script principal, o lambda_function_test.py, que integra todo o fluxo: busca os IDs dos filmes de guerra, recupera os detalhes de cada um, e por fim, salva os dados em arquivos JSON. Para isso, criei a fun√ß√£o salvar_em_json_s3, que divide os dados em lotes de at√© 100 filmes e envia cada lote como um arquivo .json para o bucket S3, organizando a estrutura no seguinte padr√£o de pastas: Raw/TMDB/JSON/movies/{ano}/{mes}/{dia}/{uuid}.json. Para o envio, utilizei o boto3 com autentica√ß√£o via dotenv, que facilita os testes locais antes de empacotar a fun√ß√£o para execu√ß√£o na AWS Lambda.

Antes de fazer o deploy final na AWS, testei toda a l√≥gica localmente com o comando lambda_handler() dentro do lambda_function_test.py. Isso permitiu validar tanto a comunica√ß√£o com a API do TMDB quanto a escrita dos arquivos JSON no bucket data-compass-ana, garantindo que o volume de dados, as autentica√ß√µes e a estrutura de pastas estavam funcionando corretamente.


| Arquivo | Link |
|--------|------|
| lambda_function_test.py | [üîó lambda_function_test.py ](./Etapa2/lambda_function_test/lambda_function_test.py) |
| tmdb_client_test.py | [üîó tmdb_client_test.py ](./Etapa2/lambda_function_test/tmdb_client_test.py) |
<br>

![Evid√™ncia 5](../Evid√™ncias/evidencia5.png)


**Lambda_layer_docker**

Como a fun√ß√£o Lambda precisa da biblioteca requests e ela n√£o est√° inclu√≠da por padr√£o no ambiente do AWS Lambda, criei uma layer customizada. Para isso, escrevi um Dockerfile baseado em public.ecr.aws/lambda/python:3.9 que instala a biblioteca requests em um diret√≥rio chamado python e a compacta como requests-layer.zip.

Esse processo garantiu que a biblioteca ficasse no formato adequado para ser utilizada como uma Lambda Layer. Posteriormente, acessei o console da AWS, fui at√© a se√ß√£o "Layers", criei uma nova layer chamada requests-layer, fiz o upload do arquivo .zip e finalizei associando essa layer √† fun√ß√£o Lambda.

| Arquivo | Link |
|--------|------|
| Dockerfile | [üîó Dockerfile ](./Etapa2/lambda_layer_docker/Dockerfile) |
| requirements.txt | [üîó requirements.txt ](./Etapa2/lambda_layer_docker/requirements.txt) |
<br>

![Evid√™ncia 6](../Evid√™ncias/evidencia6.png)

**Lambda_function**

Nessa pasta, criei as duas vers√µes finais dos arquivos tmdb_client.py e lambda_function.py, adaptadas para o ambiente de execu√ß√£o da Lambda. Nesse novo c√≥digo, substitu√≠ o uso do dotenv pela leitura direta de vari√°veis de ambiente, seguindo as boas pr√°ticas de seguran√ßa e compatibilidade com a Lambda. O fluxo l√≥gico permaneceu o mesmo: busca dos IDs, coleta de detalhes dos filmes, e envio de arquivos JSON organizados por data para o bucket S3. Com os arquivos prontos, gerei o pacote compactado tmdb_lambda_package.zip.

| Arquivo | Link |
|--------|------|
| lambda_function.py | [üîó lambda_function.py ](./Etapa2/lambda_function/lambda_function.py) |
| tmdb_client.py | [üîó tmdb_client.py ](./Etapa2/lambda_function/tmdb_client.py) |
| tmdb_lambda_package.zip | [üîó tmdb_lambda_package.zip ](./Etapa2/lambda_function/tmdb_lambda_package.zip) |
<br>


No console da AWS, acessei o servi√ßo Lambda e criei uma nova fun√ß√£o personalizada chamada tmdb-fetcher-function. Escolhi a op√ß√£o "Criar do zero", selecionei a linguagem Python 3.9, e utilizei uma role de execu√ß√£o personalizada. Ap√≥s a cria√ß√£o da fun√ß√£o, fiz upload do arquivo tmdb_lambda_package.zip. contendo os arquivos lambda_function.py e tmdb_client.py. Al√©m disso, associei a layer √† fun√ß√£o criada.

![Evid√™ncia 7](../Evid√™ncias/evidencia7.png)

Ap√≥s isso, defini duas vari√°veis de ambiente essenciais diretamente no console:
- TMDB_BEARER_TOKEN: token de autentica√ß√£o da API do TMDB
- S3_BUCKET_NAME: data-compass-ana

Ap√≥s configurar a fun√ß√£o Lambda, ela inicialmente n√£o conseguiu interagir com o bucket S3. Para resolver isso, acessei a aba de permiss√µes da fun√ß√£o Lambda e editei sua fun√ß√£o de execu√ß√£o. Atribu√≠ √† fun√ß√£o a pol√≠tica gerenciada, garantindo permiss√µes suficientes para leitura e escrita no bucket data-compass-ana.

![Evid√™ncia 8](../Evid√™ncias/evidencia8.png)

A fun√ß√£o apresentou outro erro tamb√©m por exceder o tempo de execu√ß√£o que tinha definido de 2 minutos.

![Evid√™ncia 9](../Evid√™ncias/evidencia9.png)

Para resolver isso, aumentei o tempo limite da fun√ß√£o para 10 minutos e tamb√©m ajustei a mem√≥ria alocada para 512 MB. Ap√≥s esses ajustes, executei a fun√ß√£o com sucesso, coletando os dados de todos os filmes (574 no total) e armazenando os arquivos JSON no bucket conforme esperado.

![Evid√™ncia 10](../Evid√™ncias/evidencia10.png)

Com tudo configurado, executei a fun√ß√£o diretamente pelo console da AWS. O processo foi iniciado com a mensagem ‚ÄúBuscando IDs de filmes de guerra...‚Äù, seguido da coleta dos filmes, enriquecimento dos dados, e upload de m√∫ltiplos arquivos JSON no S3.
A execu√ß√£o foi conclu√≠da com sucesso, e os arquivos foram armazenados conforme o prefixo planejado. Esta etapa marcou o sucesso da ingest√£o de dados da API externa, organizados em formato JSON no n√≠vel Raw do Data Lake.

![Evid√™ncia 11](../Evid√™ncias/evidencia11.png)
![Evid√™ncia 12](../Evid√™ncias/evidencia12.png)



