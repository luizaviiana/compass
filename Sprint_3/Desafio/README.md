# ğŸš€ Desafio 

## ğŸ“Œ Resumo

Nesse desafio, 

Neste desafio da Sprint 3, realizei o processamento de dados do arquivo "concert_tours_by_women.csv" sobre turnÃªs de artistas femininas, com o objetivo de limpar, transformar e analisar os dados, alÃ©m de gerar visualizaÃ§Ãµes grÃ¡ficas e reponder algumas questÃµes.

Para isso, realizei um script em python chamado *etl.py* que fez a limpeza dos dados. Em seguida, desenvolvi um segundo script, o *job.py* para realizar anÃ¡lises e gerar respostas e grÃ¡ficos com auxÃ­lio das bibliotecas Pandas e Matplotlib.

Todo o projeto foi containerizado com Docker e orquestrado com Docker Compose, simulando um ambiente realista de execuÃ§Ã£o em mÃºltiplos containers, com compartilhamento de dados via volume. O desenvolvimento foi organizado em etapas, desde a prototipaÃ§Ã£o inicial atÃ© a execuÃ§Ã£o final integrada, detalhadas nas seÃ§Ãµes a seguir.


## ğŸ—‚ï¸ SumÃ¡rio 

1. [Etapa 1](#etapa-1)
2. [Etapa 2](#etapa-2)
3. [Etapa 3](#etapa-3)
4. [Etapa 4](#etapa-4)
5. [Etapa 5](#etapa-5)

<br>

---

# Etapa 1

Nessa etapa do desafio, foi realizada a leitura de um arquivo oriundo de webscraping, o csv *concert_tours_by_women.csv* em que possui dados referentes Ã s turnÃªs de cantoras. Assim,  fiz um tratamento de dados inicial para posteriormente analisar as informaÃ§Ãµes contidas no mesmo. Segui uma sequÃªncia de aÃ§Ãµes no Jupyter Notebook denominado *etl.ipynb* onde usei como protÃ³tipo do cÃ³digo para detalhar um pouco mais textualmente sobre os cÃ³digos que usei para execuÃ§Ã£o dos passos, alÃ©m de ir acompanhando os resultados. ApÃ³s finalizar, passei o cÃ³digo completo para o *etl.py*, segui o modelo da imagem fornecida no desafio para que ao final fosse obtido o arquivo *csv_limpo.csv*. Para verificar se ocorreu tudo certo coloquei o comando "python etl.py" no terminal onde me retornou o *csv_limpo.csv*.


| Arquivo | Link |
|--------|------|
| concert_tours_by_women | [ğŸ”— concert_tours_by_women](./Etapa1/concert_tours_by_women.csv) |
| etl.ipynb| [ğŸ”— etl.ipynb](./Etapa1/etl.ipynb) |
| etl.py | [ğŸ”— etl.py](./Etapa1/etl.py) |
| csv_limpo | [ğŸ”— csv_limpo](./Etapa1/csv_limpo.csv) |

<br>

*Resultado final:*

![EvidÃªncia 1](../EvidÃªncias/EvidÃªncia1.png)

<br>

---

# Etapa 2

Nessa 2Â° etapa do desafio, serÃ¡ realizada a leitura do arquivo "csv_limpo.csv" que limpamos na Etapa 1 e, logo apÃ³s, realizado o processamento de dados para gerar as respostas das perguntas Q1, Q2, Q3, Q4 e Q5 na formataÃ§Ã£o pedida. Assim, como o arquivo jÃ¡ estava tratado eu nÃ£o precisei realizar o tratamento, porÃ©m, usei o Jupyter Notebook *job.ipynb* para jÃ¡ poder ir visualizando os resultados das perguntas e tambÃ©m detalhar o meu passo a passo. ApÃ³s finalizar, passei o cÃ³digo completo para o *job.py*. Para verificar se ocorreu tudo certo coloquei o comando "python job.py" no terminal onde me retornou os arquivos *respostas.txt*, *Q4.png* e *Q5.png*.


| Arquivo | Link |
|--------|------|
| csv_limpo | [ğŸ”— csv_limpo](./Etapa2/csv_limpo.csv) |
| job.ipynb| [ğŸ”— job.ipynb](./Etapa2/job.ipynb) |
| job.py | [ğŸ”— job.py](./Etapa2/job.py) |

<br>

> *Resultado final:*

- [respostas.txt](./Etapa2/job.py) 

![EvidÃªncia 2](../EvidÃªncias/EvidÃªncia2.png)

<br>

- Q4.png 

![EvidÃªncia 3](../EvidÃªncias/EvidÃªncia4.png)

<br>

- Q5.png 

![EvidÃªncia 4](../EvidÃªncias/EvidÃªncia5.png)


<br>

---
# Etapa 3

Nesta etapa do desafio, o objetivo foi criar um container Docker para executar o script etl.py, responsÃ¡vel pela limpeza dos dados. Para isso, organizei os arquivos na pasta Etapa3, incluindo o script etl.py, o arquivo original concert_tours_by_women.csv, o requirements.txt com a biblioteca pandas, e uma pasta volume destinada ao armazenamento do arquivo de saÃ­da csv_limpo.csv.

Criei um Dockerfile utilizando a imagem base python:3.10-slim, que oferece um ambiente enxuto e funcional. Depois disso, defini o diretÃ³rio de trabalho do container com o comando WORKDIR /app, estabelecendo que todas as aÃ§Ãµes subsequentes aconteceriam dentro desse diretÃ³rio. A partir daÃ­, copiei para dentro do container os arquivos essenciais para a execuÃ§Ã£o: o script etl.py, o CSV original e o requirements.txt. Com os arquivos no ambiente do container, instalei as dependÃªncias usando o comando de instalaÃ§Ã£o do pip, que leu o requirements.txt e instalou o pacote necessÃ¡rio, evitando o uso de cache para manter a imagem mais leve.

Para garantir que o arquivo de saÃ­da pudesse ser acessado externamente, adicionei um volume no caminho /app/volume. Esse volume foi pensado para permitir que a saÃ­da do processo de ETL, o arquivo csv_limpo.csv, fosse salva fora do container, possibilitando o uso em etapas posteriores. TambÃ©m modifiquei o script etl.py para que escrevesse a saÃ­da diretamente dentro da pasta /app/volume.

ApÃ³s configurar o Dockerfile, utilizei o terminal para construir a imagem com o comando docker build -t etl ., dentro da pasta Etapa3. Com a imagem criada, executei o container utilizando docker run -v "$(Get-Location)\volume:/app/volume" etl, que conecta a pasta local volume ao volume do container, garantindo o compartilhamento do arquivo de saÃ­da. Ao final da execuÃ§Ã£o, confirmei que o arquivo csv_limpo.csv foi gerado corretamente na pasta local volume, validando que o processo de ETL foi executado com sucesso dentro do ambiente Docker.

| Arquivo | Link |
|--------|------|
| concert_tours_by_women | [ğŸ”— concert_tours_by_women](./Etapa3/concert_tours_by_women.csv) |
| etl.py | [ğŸ”— etl.py](./Etapa3/etl.py) |
| Dockerfile | [ğŸ”— Dockerfile](./Etapa3/Dockerfile) |
| Requirements | [ğŸ”— Requirements](./Etapa3/Requirements.txt) |
| csv_limpo | [ğŸ”— csv_limpo](./Etapa3/csv_limpo) |

<br>

> *Resultado final:*

- ConstruÃ§Ã£o da Imagem

![EvidÃªncia 5](../EvidÃªncias/EvidÃªncia5.png)

<br>

- ExecuÃ§Ã£o do Container

![EvidÃªncia 6](../EvidÃªncias/EvidÃªncia6.png)

<br>

---
# Etapa 4

Nesta etapa do desafio, o objetivo foi criar um container Docker para executar o script job.py, responsÃ¡vel pelo processamento dos dados e geraÃ§Ã£o das respostas e grÃ¡ficos com base no arquivo csv_limpo.csv, produzido na etapa anterior. Para isso, organizei a pasta Etapa4 incluindo o arquivo csv_limpo.csv, o script job.py, o requirements.txt com as bibliotecas pandas e matplotlib e uma pasta volume onde seriam salvos os arquivos de saÃ­da.

Criei um Dockerfile com base na imagem python:3.11-slim, que oferece leveza e compatibilidade com as bibliotecas utilizadas. Defini o diretÃ³rio de trabalho como /app e copiei os arquivos para dentro do container: o script job.py, o CSV limpo e o requirements.txt. Em seguida, instalei as dependÃªncias com pip, sem armazenar cache, para manter a imagem final mais leve. TambÃ©m configurei um volume em /app/volume e adaptei o script job.py para que todos os arquivos gerados, o respostas.txt, Q4.png e Q5.png, fossem salvos diretamente nessa pasta.

Com a estrutura pronta, utilizei o comando pipreqs para gerar o requirements.txt com as bibliotecas exatas utilizadas. Depois, construÃ­ a imagem Docker com docker build -t job . e executei o container utilizando docker run -v "$(Get-Location)\volume:/app/volume" job, conectando a pasta local volume ao volume do container. ApÃ³s a execuÃ§Ã£o, os arquivos de saÃ­da foram corretamente salvos na pasta local.



| Arquivo | Link |
|--------|------|
| csv_limpo | [ğŸ”— csv_limpo](./Etapa4/csv_limpo) |
| job.py | [ğŸ”— job.py](./Etapa4/job.py) |
| Dockerfile | [ğŸ”— Dockerfile](./Etapa4/Dockerfile) |
| Requirements | [ğŸ”— Requirements](./Etapa4/Requirements.txt) |


<br>

> *Resultado final:*

- ConstruÃ§Ã£o da Imagem

![EvidÃªncia 7](../EvidÃªncias/EvidÃªncia7.png)

<br>

- ExecuÃ§Ã£o do Container

![EvidÃªncia 8](../EvidÃªncias/EvidÃªncia8.png)

<br>

---

# Etapa 5

Nesta etapa do desafio, foi solicitado que a execuÃ§Ã£o do processo completo ocorresse exclusivamente por meio de um arquivo docker-compose, conectando os dois containers construÃ­dos nas etapas anteriores. Para isso, organizei todos os arquivos necessÃ¡rios dentro da pasta da Etapa 5, incluindo o arquivo original concert_tours_by_women.csv, o script etl.py da Etapa 3 e o job.py da Etapa 4. TambÃ©m criei um novo arquivo requirements.txt contendo todas as dependÃªncias utilizadas pelos dois scripts.

A estrutura do docker-compose.yml foi construÃ­da definindo dois serviÃ§os principais que representam cada etapa do processo. O primeiro serviÃ§o, chamado etl, Ã© responsÃ¡vel por construir e executar o container que realiza a etapa de extraÃ§Ã£o e transformaÃ§Ã£o dos dados, utilizando o Dockerfile presente na Etapa 3. JÃ¡ o segundo serviÃ§o, chamado job, Ã© encarregado de rodar o container responsÃ¡vel pelo processamento dos dados e geraÃ§Ã£o dos arquivos requeridos, com base no Dockerfile da Etapa 4. Ambos os serviÃ§os compartilham um volume chamado /volume, que funciona como um diretÃ³rio comum para que os arquivos de saÃ­da gerados pela etapa etl possam ser acessados diretamente pela etapa job. AlÃ©m disso, defini a dependÃªncia entre os serviÃ§os por meio da instruÃ§Ã£o depends_on, garantindo que o serviÃ§o job sÃ³ inicie apÃ³s a execuÃ§Ã£o do etl.

Para executar utilizei o comando docker-compose up --build no terminal. Com isso, o pipeline foi executado com sucesso, gerando os arquivos finais esperados na pasta /volume: csv_limpo.csv, respostas.txt, Q4.png e Q5.png.

| Arquivo | Link |
|--------|------|
| concert_tours_by_women | [ğŸ”— concert_tours_by_women](./Etapa5/concert_tours_by_women.csv) |
| etl.py | [ğŸ”— etl.py](./Etapa5/etl.py) |
| job.py | [ğŸ”— job.py](./Etapa5/job.py) |
| docker-compose | [ğŸ”— docker-compose](./Etapa5/docker-compose.yml) |
| Requirements | [ğŸ”— Requirements](./Etapa5/Requirements.txt) |
| csv_limpo | [ğŸ”— csv_limpo](./Etapa5/volume/csv_limpo.csv) |
| respostas.txt| [ğŸ”— respostas.txt](./Etapa5/volume/respostas.txt) |
| Q4.png | [ğŸ”— Q4.png](./Etapa5/volume/Q4.png) |
| Q5.png | [ğŸ”— Q5.png](./Etapa5/volume/Q5.png) |

<br>

> *Resultado final:*

- ConstruÃ§Ã£o da Imagem

![EvidÃªncia 9](../EvidÃªncias/EvidÃªncia9.png)

<br>