# üöÄ Sprint 6

## üìå Resumo

Durante a Sprint 6, aprofundei meus conhecimentos pr√°ticos em ferramentas essenciais da AWS para engenharia de dados. Trabalhei principalmente com o AWS Glue, usando crawlers para catalogar os dados brutos e convertidos no Data Catalog, jobs para realizar transforma√ß√µes e convers√µes dos arquivos JSON para o formato Parquet. Tamb√©m utilizei o Glue para organizar e monitorar os fluxos de execu√ß√£o, garantindo o particionamento por data e a padroniza√ß√£o das estruturas de diret√≥rios no S3. Com o Athena, realizei consultas SQL sobre os dados j√° estruturados, validando a integridade dos registros e extraindo insights para a pr√≥xima etapa anal√≠tica do projeto. Essa experi√™ncia foi fundamental para consolidar o entendimento do funcionamento de um Data Lake na pr√°tica.


- **Data & Analytics**: atrav√©s do D&A, pude executar os exerc√≠cios definidos para essa sprint, o que auxiliou a colocar em pr√°tica os conhecimentos adquiridos, o Lab AWS Glue foi essencial e ajudou muito para a execu√ß√£o do desafio posteriormente. Al√©m disso, tamb√©m forneceu as principais informa√ß√µes para seguir com a realiza√ß√£o do desafio.

- **AWS - Tutoriais T√©cnicos - Analytics**: Atrav√©s da playlist de v√≠deos pude acompanhar a aplica√ß√£o de alguns servi√ßos da AWS como o Athena, Glue, Quicksight e como s√£o suas arquiteturas, o que auxiliou a incrementar minha base de conhecimento sobre a AWS.

- **Fundamentals of Analytics on AWS ‚Äì Part 2 (Portugu√™s)**: Neste curso, aprofundei meus conhecimentos sobre data lakes, data warehouses e arquiteturas de dados modernas na AWS, complementando os conceitos da Parte 1. Aprendi como servi√ßos como AWS Lake Formation, Amazon Redshift, Amazon S3, AWS Glue e Amazon Athena s√£o usados para projetar solu√ß√µes escal√°veis de an√°lise de dados.

- **AWS Glue Getting Started (Portugu√™s)**: Por meio desse curso, adquiri conhecimentos pr√°ticos sobre o AWS Glue, o servi√ßo de integra√ß√£o de dados serverless da AWS para ETL. Aprendi como ele simplifica a prepara√ß√£o, cataloga√ß√£o e transforma√ß√£o de dados para an√°lises, machine learning e desenvolvimento de aplica√ß√µes.

ü§î *Reflex√µes*

A Sprint 6 foi bastante desafiadora, j√° que trabalhei com ferramentas novas na AWS. Por√©m, agora j√° sinto uma maior confian√ßa e melhor adaptada aos desafios, as dificuldades que enfrentei consegui super√°-las com o apoio do meu squad. √Ä medida que fui avan√ßando nos exerc√≠cios, fui ganhando mais confian√ßa, e concluir o desafio final foi bastante gratificante. Al√©m disso, me sinto muito grata por todo o suporte que venho recebendo dos monitores, da Scrum Master, do meu time e dos demais membros da PB.



<br>

---

## üóÇÔ∏è Sum√°rio 

1. [Desafio](#desafio)

2. [Exerc√≠cios](#exerc√≠cios)
    - 2.1 [Gera√ß√£o e massa de dados](#21-gera√ß√£o-e-massa-de-dados)
    - 2.2 [Apache Spark](#22-apache-spark)
    - 2.3 [Lab AWS Glue](#23-lab-aws-glue)

3. [Certificados](#certificados)

<br>

---

# [Desafio](./Desafio/) 

Nesse desafio dei continuidade √† terceira etapa do projeto, o foco foi o processamento e organiza√ß√£o dos dados na camada Trusted do Data Lake, utilizando o servi√ßo AWS Glue para transformar os arquivos da camada Raw. Para isso, criei dois jobs em Glue, um para processar os arquivos CSV e outro para os dados da API TMDB no formato JSON, convertendo-os para o formato Parquet e organizando-os conforme o padr√£o de estrutura definido para o projeto.

Em seguida, configurei e executei crawlers para catalogar esses dados na camada Trusted, garantindo a cria√ß√£o das tabelas no Glue Data Catalog dentro do database dedicado ao projeto. Finalizei a etapa com a valida√ß√£o dos dados via AWS Athena, conferindo a exist√™ncia das tabelas e a possibilidade de consultas SQL para suportar an√°lises futuras. Essa etapa consolidou a organiza√ß√£o da camada Trusted, facilitando o acesso e a consulta dos dados de forma eficiente e padronizada, al√©m de preparar a base para os pr√≥ximos passos do projeto. Os arquivos utilizados para a realiza√ß√£o do desafio est√£o organizados em pastas por etapas, acompanhando as fases do desenvolvimento, e podem ser encontrados na *Pasta Desafio*. As evid√™ncias do processo est√£o armazenadas na *Pasta Evid√™ncias*. Para um detalhamento completo do desafio, recomendo consultar o README da pasta *Readme Desafio*. Seguem os links:

- [Pasta Desafio](./Desafio/) 
- [Pasta Evid√™ncias](./Evid√™ncias/)
- [Readme Desafio](./Desafio/README.md)


<br>

---

# [Exerc√≠cios](./Exerc√≠cios/)

## 2.1 Gera√ß√£o e massa de dados

Neste exerc√≠cio, a proposta foi gerar dados em Python que ser√£o utilizados futuramente em atividades com o framework Apache Spark. O exerc√≠cio foi dividido em tr√™s etapas, todas voltadas √† manipula√ß√£o de listas e gera√ß√£o de arquivos, com foco em aquecer o uso da linguagem Python antes de processamentos mais pesados com Spark. Para a execu√ß√£o, utilizei o Jupyter Notebook para desenvolver e validar as etapas 1, 2 e o teste da etapa 3, aproveitando a visualiza√ß√£o interativa do ambiente. J√° a etapa 3 completa, que envolve a gera√ß√£o de 10 milh√µes de nomes, foi implementada em um script Python separado (Etapa3.py) e executada via terminal, garantindo maior performance no processamento e grava√ß√£o dos dados. Abaixo, detalho os passos realizados em cada etapa, os c√≥digos utilizados e as decis√µes t√©cnicas adotadas.

| Arquivo | Link |
|--------|------|
| Exerc√≠cio1.ipynb | [üîó Exerc√≠cio1.ipynb](./Exerc√≠cios/Exerc√≠cio1/Exerc√≠cio1.ipynb) |
| Etapa3.py | [üîó Etapa3.py](./Exerc√≠cios/Exerc√≠cio1/Etapa3.py) |
<br>

>Resolu√ß√£o:

Etapa 1 ‚Äì Utilizei a biblioteca random para gerar uma lista contendo 250 n√∫meros inteiros aleat√≥rios entre 1 e 1000. Em seguida, apliquei o m√©todo .reverse() para inverter a ordem dos elementos dessa lista. Para finalizar, imprimi o resultado para valida√ß√£o.

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia1.png)

Etapa 2 ‚Äì Na segunda etapa, ainda utilizando o Jupyter Notebook, declarei manualmente uma lista contendo 20 nomes de animais. Em seguida, utilizei o m√©todo .sort() para orden√°-los em ordem alfab√©tica e imprimi cada item individualmente utilizando list comprehension. Por fim, salvei o conte√∫do da lista em um arquivo de texto chamado animais_ordenados.txt, com um animal por linha, utilizando o comando with open() para abrir e escrever o arquivo de forma eficiente.

| Arquivo | Link |
|--------|------|
| animais_ordenados.txt | [üîó animais_ordenados.txt](./Exerc√≠cios/Exerc√≠cio1/animais_ordenados.txt) |
<br>

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia2.png)

Etapa 3 ‚Äì Para validar a l√≥gica antes de rodar a vers√£o completa da terceira etapa, iniciei com uma vers√£o reduzida no Jupyter Notebook. Instalei a biblioteca names atrav√©s do terminal com o comando pip install names, e em seguida utilizei essa biblioteca para gerar 1.000 nomes √∫nicos. A partir dessa base, gerei uma lista com 10 mil nomes aleat√≥rios usando random.choice() e salvei os dados em um arquivo chamado nomes_aleatorios_teste.txt, com um nome por linha. Esse teste me permitiu garantir que toda a l√≥gica de gera√ß√£o e grava√ß√£o estava funcionando corretamente, antes de executar o processo completo no terminal.

| Arquivo | Link |
|--------|------|
| nomes_aleatorios_teste.txt | [üîó nomes_aleatorios_teste.txt](./Exerc√≠cios/Exerc√≠cio1/nomes_aleatorios_teste.txt) |
<br>

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia3.png)
<br>

Ap√≥s validar a l√≥gica da gera√ß√£o de nomes com a vers√£o de teste no Jupyter Notebook, desenvolvi a vers√£o final da terceira etapa em um script separado chamado Etapa3.py. Neste script, utilizei a biblioteca names para gerar 39.080 nomes √∫nicos e, a partir dessa base, criei 10 milh√µes de nomes aleat√≥rios com a fun√ß√£o random.choice(). Para garantir performance na escrita, abri o arquivo nomes_aleatorios.txt uma √∫nica vez utilizando o comando with open(), gravando todos os nomes de forma sequencial, um por linha. Executei esse script diretamente no terminal com o comando python Etapa3.py, o que permitiu melhor desempenho no processamento e na gera√ß√£o do arquivo final. O arquivo nomes_aleatorios.txt n√£o est√° no reposit√≥rio por exceder o limite de tamanho do GitHub, mas que ele pode ser facilmente reproduzido executando o script Etapa3.py.

| Arquivo | Link |
|--------|------|
| Etapa3.py | [üîó Etapa3.py](./Exerc√≠cios/Exerc√≠cio1/Etapa3.py) |
<br>

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia4.png)

<br>

## 2.2 Apache Spark

Neste exerc√≠cio, o objetivo foi praticar a manipula√ß√£o de dados em larga escala utilizando o framework Apache Spark por meio da biblioteca PySpark. As atividades foram organizadas em etapas sequenciais, onde a cada passo novas transforma√ß√µes e consultas foram realizadas sobre o DataFrame . Inicialmente, configurei o ambiente de execu√ß√£o, garantindo a integra√ß√£o correta entre o Python, o Java e o Spark, e criei um script de execu√ß√£o autom√°tica *run_pyspark.ps1* para facilitar os testes no terminal do Windows. A leitura e a estrutura√ß√£o dos dados foram feitas a partir do arquivo nomes_aleatorios.txt, e o DataFrame df_nomes foi enriquecido com colunas adicionais como Escolaridade, Pa√≠s, Ano de Nascimento e Gera√ß√£o.

A seguir, descrevo cada etapa do processo, incluindo os c√≥digos desenvolvidos:

| Arquivo | Link |
|--------|------|
| Exercicio2.py | [üîó Exercicio2.py](./Exerc√≠cios/Exerc√≠cio2/Exercicio2.py) |
| run_pyspark.ps1 | [üîó run_pyspark.ps1](./Exerc√≠cios/Exerc√≠cio2/run_pyspark.ps1) |
<br>

>Resolu√ß√£o:

Etapa 1 ‚Äì Criei o script Etapa1.py onde importei as bibliotecas SparkSession, SparkContext e SQLContext. Em seguida, criei a sess√£o Spark, e utilizei spark.read.csv para carregar o arquivo nomes_aleatorios.txt, que est√° localizado na pasta do exerc√≠cio 1, o caminho foi ajustado corretamente para acessar o arquivo. Por fim, visualizei as 5 primeiras linhas com o m√©todo .show(5).

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia1.png)

Etapa 2 ‚Äì Criei o script Exercicio2.py, onde continuei utilizando a sess√£o Spark iniciada anteriormente. Inicialmente, carreguei novamente o arquivo nomes_aleatorios.txt usando o mesmo caminho relativo. Como o Spark n√£o reconheceu automaticamente os nomes das colunas, a coluna foi nomeada como _c0. Renomeei essa coluna para Nomes utilizando o m√©todo withColumnRenamed. Em seguida, utilizei o m√©todo printSchema() para visualizar a estrutura do dataframe, confirmando que a coluna agora se chama Nomes e est√° com o tipo string. Por fim, usei o m√©todo .show(10) para exibir as 10 primeiras linhas do dataframe, acessando a coluna pelo formato de √≠ndice, conforme exigido no enunciado.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia2.png)

Etapa 3 ‚Äì Ap√≥s carregar e renomear a coluna, adicionei uma nova coluna chamada Escolaridade. Para isso, criei uma coluna auxiliar rand_num com valores aleat√≥rios inteiros de 0 a 2, usando a fun√ß√£o rand com semente fixa para reprodutibilidade e a fun√ß√£o floor. Depois, com o m√©todo when, atribu√≠ os valores "Fundamental", "M√©dio" e "Superior" conforme o n√∫mero aleat√≥rio gerado. Removi a coluna auxiliar rand_num em seguida. Para conferir o resultado, utilizei printSchema() para ver a estrutura do dataframe e select().show() para exibir as 10 primeiras linhas com os nomes e escolaridades.

Al√©m do script Python, criei o arquivo run_pyspark.ps1 para facilitar a execu√ß√£o dos scripts PySpark, configurando a vari√°vel de ambiente JAVA_HOME com o JDK 8 antes de rodar o c√≥digo. 

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia3.png)

Etapa 4 ‚Äì Nesta etapa, adicionei ao dataframe uma nova coluna chamada Pais, contendo valores aleat√≥rios entre 13 pa√≠ses da Am√©rica do Sul. Para isso, criei uma coluna auxiliar chamada rand_pais, gerando n√∫meros inteiros de 0 a 12 com a fun√ß√£o rand combinada com floor, e usando uma semente fixa. Em seguida, utilizei a fun√ß√£o expr do PySpark para construir uma express√£o CASE WHEN que mapeia cada n√∫mero para um pa√≠s da lista. Ap√≥s o mapeamento, removi a coluna auxiliar rand_pais e finalizei exibindo o esquema do DataFrame com printSchema() e mostrando as 10 primeiras linhas com show().

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia4.png)

Etapa 5 ‚Äì Adicionei uma nova coluna chamada AnoNascimento ao dataframe, utilizei a fun√ß√£o rand com semente fixa e multipliquei por 66, que corresponde √† quantidade de anos entre 1945 e 2010. Em seguida, apliquei a fun√ß√£o floor para obter n√∫meros inteiros e somei 1945, garantindo que os valores finais ficassem no intervalo desejado, depois atribu√≠ esse resultado diretamente √† nova coluna utilizando withColumn.

![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia5.png)

Etapa 6 ‚Äì Criei um novo dataframe chamado df_select contendo apenas as pessoas nascidas a partir de 2001, ou seja, neste s√©culo. Para isso, utilizei o m√©todo select para extrair as colunas "Nomes" e "AnoNascimento" do df_nomes, aplicando em seguida o m√©todo where com a condi√ß√£o AnoNascimento >= 2001.  Por fim, utilizei o m√©todo show(10) para visualizar os 10 primeiros nomes resultantes desse novo dataframe.

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia6.png)

Etapa 7 ‚Äì Para repetir a sele√ß√£o das pessoas que nasceram neste s√©culo usando Spark SQL, primeiro registrei o dataframe df_nomes como uma tabela tempor√°ria chamada pessoas com o m√©todo createOrReplaceTempView(). Em seguida, executei uma consulta SQL para selecionar os nomes e anos de nascimento onde o ano √© maior ou igual a 200, o resultado foi armazenado no dataframe df_select. Dessa forma, realizei a filtragem usando a linguagem SQL diretamente no Spark.

![Evid√™ncia 7](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia7.png)

Etapa 8 ‚Äì Nesta etapa, utilizei o m√©todo filter do DataFrame df_nomes para filtrar as pessoas que pertencem √† gera√ß√£o Millennials. Para isso, apliquei uma condi√ß√£o l√≥gica com operadores >= e <= na coluna AnoNascimento. Em seguida, utilizei o m√©todo count() para contar quantas linhas atendem a essa condi√ß√£o e imprimi o resultado no console.

![Evid√™ncia 8](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia8.png)

Etapa 9 ‚Äì Utilizei Spark SQL para contar quantas pessoas pertencem √† gera√ß√£o Millennials, assim, usei novamente a temp view chamada pessoas, criada a partir do df_nomes. Em seguida, escrevi uma query SQL usando o comando SELECT COUNT(*) com a cl√°usula WHERE AnoNascimento BETWEEN 1980 AND 1994. O resultado foi exibido com o m√©todo .show(), apresentando a quantidade total de registros que atendem ao crit√©rio da gera√ß√£o Millennials.

![Evid√™ncia 9](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia9.png)

Etapa 10 ‚Äì Nesta etapa, utilizei Spark SQL para contar a quantidade de pessoas de cada pa√≠s pertencentes a diferentes gera√ß√µes. Para isso, primeiro registrei o dataframe df_nomes como uma tabela tempor√°ria chamada pessoas. Em seguida, constru√≠ uma consulta SQL utilizando a cl√°usula CASE para categorizar cada pessoa em uma das quatro gera√ß√µes dadas, com uma op√ß√£o adicional chamada "Outros" para qualquer caso fora dessas faixas. A consulta agrupou os dados por pa√≠s e gera√ß√£o e contou o n√∫mero de registros em cada combina√ß√£o. Por fim, ordenei o resultado por pa√≠s, gera√ß√£o e quantidade, e exibi as linhas com o m√©todo .show().

![Evid√™ncia 10](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia10.png)

<br>

## 2.3 Lab AWS Glue

Neste exerc√≠cio, fui guiada no laborat√≥rio AWS para realizar a constru√ß√£o de um processo de ETL simplificado utilizando o servi√ßo AWS Glue. A seguir, descrevo cada etapa do processo, incluindo os c√≥digos desenvolvidos:

| Arquivo | Link |
|--------|------|
| Etapa1.ipynb | [üîó Etapa1.ipynb](./Exerc√≠cios/Exerc√≠cio3/Etapa1.ipynb) |
<br>

>Resolu√ß√£o:

Etapa 1 ‚Äì Nesta etapa, criei um script em Python para enviar o arquivo nomes.csv para o bucket compass-ana-lab-glue na AWS S3, no caminho lab-glue/input/nomes.csv. Utilizei as bibliotecas boto3, dotenv e os para configurar o cliente S3 com as credenciais armazenadas em um arquivo .env. O script verifica se o bucket j√° existe e, caso n√£o exista, o cria dinamicamente de acordo com a regi√£o definida (us-east-1). Em seguida, realiza o upload do arquivo para o caminho especificado.

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia1.png)

Etapa 2 ‚Äì Fiz a cria√ß√£o da a role AWSGlueServiceRole-Lab4 no console IAM da AWS. A role foi configurada para ser assumida pelo servi√ßo AWS Glue e recebeu permiss√µes para facilitar a execu√ß√£o de jobs no ambiente de laborat√≥rio. Durante a cria√ß√£o, associei as seguintes policies gerenciadas pela AWS: AmazonS3FullAccess, AWSLakeFormationDataAdmin, AWSGlueConsoleFullAccess e CloudWatchFullAccess. Com isso, o Glue poder√° acessar o S3, utilizar o Lake Formation, executar notebooks e gerar logs no CloudWatch.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia2.png)

Etapa 3 ‚Äì Configurei o AWS Glue pela op√ß√£o ‚ÄúPrepare your account for AWS Glue‚Äù, criando a role padr√£o recomendada e concedendo acesso total ao S3, permitindo que o Glue execute jobs, crawlers e notebooks com acesso aos dados armazenados no S3, al√©m de registrar logs e operar com permiss√µes adequadas no ambiente de laborat√≥rio.

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia3.png)

Etapa 4 ‚Äì Acessei o servi√ßo AWS Lake Formation e, ao primeiro acesso, adicionei permiss√µes administrativas √† minha conta clicando em Add myself. Em seguida, criei um database no Data Catalog do Glue, com o nome glue-lab, utilizando o cat√°logo padr√£o da conta.

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia4.png)

Etapa 5 ‚Äì Antes de criar o job no AWS Glue, analisei localmente o arquivo nomes.csv utilizando PySpark no Jupyter Notebook. Configurei as vari√°veis de ambiente necess√°rias para iniciar a SparkSession e executei um script que realiza a leitura do CSV, infere o schema, aplica um filtro para selecionar apenas os registros do ano de 1934 e exibe as primeiras linhas do resultado, aescrita no formato Parquet foi omitida localmente para evitar conflitos com depend√™ncias do Hadoop. Essa etapa foi essencial para validar a l√≥gica do processamento antes da execu√ß√£o na nuvem.


![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia5.png)


Ap√≥s a valida√ß√£o, criei o job job_aws_glue_lab_4 no servi√ßo AWS Glue com o objetivo de processar o arquivo nomes.csv armazenado no S3. O job foi configurado para ler o CSV, aplicar um filtro para selecionar apenas os registros referentes ao ano de 1934 e escrever o resultado no formato Parquet em um novo diret√≥rio no bucket.

Os par√¢metros do job foram definidos como:

--S3_INPUT_PATH = s3://compass-ana-lab-glue/lab-glue/input/nomes.csv
--S3_TARGET_PATH = s3://compass-ana-lab-glue/lab-glue/output/nomes_1934.parquet

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia6.png)

![Evid√™ncia 7](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia7.png)

![Evid√™ncia 8](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia8.png)


Etapa 5.2 ‚Äì Nesta etapa segui a mesma l√≥gica da anterior, iniciei testando todo o script localmente no Jupyter Notebook destinado √† etapa 5, utilizando o PySpark. Fiz isso para garantir que as transforma√ß√µes funcionassem corretamente antes de aplicar no ambiente da AWS e, assim, evitar custos desnecess√°rios. Ap√≥s validar o funcionamento, adaptei o c√≥digo para o ambiente do AWS Glue.

![Evid√™ncia 9](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia9.png)

No script do AWS Glue, iniciei o job capturando os argumentos de entrada para identificar os caminhos no S3 e o nome do job, criei os contextos necess√°rios e realizei a leitura do arquivo nomes.csv no S3 como DynamicFrame, convertendo-o para DataFrame para aplicar as transforma√ß√µes. Verifiquei o schema e converti os nomes para letras mai√∫sculas com a fun√ß√£o upper(). Em seguida, contei o total de registros e agrupei os dados por ano e sexo, somando os totais e ordenando os anos de forma decrescente. Utilizei uma janela de parti√ß√£o para identificar o nome mais frequente por sexo e ano e depois agrupei os dados novamente por ano, somando os totais e ordenando os 10 primeiros anos de forma crescente, ap√≥s isso, converti o DataFrame final de volta para DynamicFrame.


![Evid√™ncia 10](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia10.png)


Por fim, escrevi os dados transformados de volta no S3, no formato JSON, com particionamento pelas colunas sexo e ano, no caminho s3://compass-ana-lab-glue/lab-glue/frequencia_registro_nomes_eua.

![Evid√™ncia 11](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia11.png)


Etapa 6 ‚Äì Criei um crawler no AWS Glue para catalogar os dados gravados anteriormente no S3 em formato JSON, particionados por sexo e ano. Nomeei o crawler como FrequenciaRegistroNomesCrawler e apontei como fonte de dados o caminho s3://compass-ana-lab-glue/lab-glue/frequencia_registro_nomes_eua/. Configurei a role AWSGlueServiceRole-Lab4 para permiss√µes e defini como banco de destino o glue-lab, com execu√ß√£o do tipo On Demand. Ap√≥s a cria√ß√£o, executei o crawler com sucesso, e ele criou automaticamente a tabela frequencia_registro_nomes_eua no Glue Catalog.

Finalizei acessando o Athena para validar os dados catalogados, configurei o local de sa√≠da das queries como s3://compass-ana-lab-glue/lab-glue/athena-results/, e consegui executar a visualiza√ß√£o dos dados diretamente via SQL no Athena.

![Evid√™ncia 12](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia12.png)


---


# Certificados


Durante essa sprint, conclu√≠ os cursos AWS: Fundamentals of Analytics on AWS ‚Äì Part 2 (Portugu√™s) e AWS Glue Getting Started (Portugu√™s). A seguir, compartilho os certificados correspondentes:

| Certificado | Link |
|--------|------|
| Fundamentals of Analytics on AWS ‚Äì Part 2 (Portugu√™s) | [üîó Link ](./Certificados/Fundamentals%20of%20Analytics%20on%20AWS%20‚Äì%20Part%202%20(Portugu√™s).pdf) |
| AWS Glue Getting Started (Portugu√™s) | [üîó Link ](./Certificados/AWS%20Glue%20Getting%20Started%20(Portugu√™s).pdf) |
<br>
