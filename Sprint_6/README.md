# ğŸš€ Sprint 6

## ğŸ“Œ Resumo



- **Data & Analytics**: atravÃ©s do D&A, pude ...... . AlÃ©m disso, tambÃ©m forneceu as principais informaÃ§Ãµes para seguir com a realizaÃ§Ã£o dos exercÃ­cios e do desafio.

- **AWS - Tutoriais TÃ©cnicos - Analytics**: 

- **Fundamentals of Analytics on AWS â€“ Part 2 (PortuguÃªs)**: Neste curso, aprofundei meus conhecimentos sobre data lakes, data warehouses e arquiteturas de dados modernas na AWS, complementando os conceitos da Parte 1. Aprendi como serviÃ§os como AWS Lake Formation, Amazon Redshift, Amazon S3, AWS Glue e Amazon Athena sÃ£o usados para projetar soluÃ§Ãµes escalÃ¡veis de anÃ¡lise de dados.

- **AWS Glue Getting Started (PortuguÃªs)**: Por meio desse curso, adquiri conhecimentos prÃ¡ticos sobre o AWS Glue, o serviÃ§o de integraÃ§Ã£o de dados serverless da AWS para ETL. Aprendi como ele simplifica a preparaÃ§Ã£o, catalogaÃ§Ã£o e transformaÃ§Ã£o de dados para anÃ¡lises, machine learning e desenvolvimento de aplicaÃ§Ãµes.

ğŸ¤” *ReflexÃµes*




<br>

---

## ğŸ—‚ï¸ SumÃ¡rio 

1. [Desafio](#desafio)

2. [ExercÃ­cios](#exercÃ­cios)
    - 2.1 [GeraÃ§Ã£o e massa de dados](#21-geraÃ§Ã£o-e-massa-de-dados)
    - 2.2 [Apache Spark](#)
    - 2.3 [Lab AWS Glue](#)

3. [Certificados](#certificados)

<br>

---

# [Desafio](./Desafio/) 

. Os arquivos utilizados para a realizaÃ§Ã£o do desafio estÃ£o organizados em pastas por etapas, acompanhando as fases do desenvolvimento, e podem ser encontrados na *Pasta Desafio*. As evidÃªncias do processo estÃ£o armazenadas na *Pasta EvidÃªncias*. Para um detalhamento completo do desafio, recomendo consultar o README da pasta *Readme Desafio*. Seguem os links:

- [Pasta Desafio](./Desafio/) 
- [Pasta EvidÃªncias](./EvidÃªncias/)
- [Readme Desafio](./Desafio/README.md)


<br>

---

# [ExercÃ­cios](./ExercÃ­cios/)

## 2.1 GeraÃ§Ã£o e massa de dados

Neste exercÃ­cio, a proposta foi gerar dados em Python que serÃ£o utilizados futuramente em atividades com o framework Apache Spark. O exercÃ­cio foi dividido em trÃªs etapas, todas voltadas Ã  manipulaÃ§Ã£o de listas e geraÃ§Ã£o de arquivos, com foco em aquecer o uso da linguagem Python antes de processamentos mais pesados com Spark. Para a execuÃ§Ã£o, utilizei o Jupyter Notebook para desenvolver e validar as etapas 1, 2 e o teste da etapa 3, aproveitando a visualizaÃ§Ã£o interativa do ambiente. JÃ¡ a etapa 3 completa, que envolve a geraÃ§Ã£o de 10 milhÃµes de nomes, foi implementada em um script Python separado (Etapa3.py) e executada via terminal, garantindo maior performance no processamento e gravaÃ§Ã£o dos dados. Abaixo, detalho os passos realizados em cada etapa, os cÃ³digos utilizados e as decisÃµes tÃ©cnicas adotadas.

| Arquivo | Link |
|--------|------|
| ExercÃ­cio1.ipynb | [ğŸ”— ExercÃ­cio1.ipynb](./ExercÃ­cios/ExercÃ­cio1/ExercÃ­cio1.ipynb) |
| Etapa3.py | [ğŸ”— Etapa3.py](./ExercÃ­cios/ExercÃ­cio1/Etapa3.py) |
<br>

>ResoluÃ§Ã£o:

Etapa 1 â€“ Utilizei a biblioteca random para gerar uma lista contendo 250 nÃºmeros inteiros aleatÃ³rios entre 1 e 1000. Em seguida, apliquei o mÃ©todo .reverse() para inverter a ordem dos elementos dessa lista. Para finalizar, imprimi o resultado para validaÃ§Ã£o.

![EvidÃªncia 1](./ExercÃ­cios/ExercÃ­cio1/EvidÃªncias/Evidencia1.png)

Etapa 2 â€“ Na segunda etapa, ainda utilizando o Jupyter Notebook, declarei manualmente uma lista contendo 20 nomes de animais. Em seguida, utilizei o mÃ©todo .sort() para ordenÃ¡-los em ordem alfabÃ©tica e imprimi cada item individualmente utilizando list comprehension. Por fim, salvei o conteÃºdo da lista em um arquivo de texto chamado animais_ordenados.txt, com um animal por linha, utilizando o comando with open() para abrir e escrever o arquivo de forma eficiente.

| Arquivo | Link |
|--------|------|
| animais_ordenados.txt | [ğŸ”— animais_ordenados.txt](./ExercÃ­cios/ExercÃ­cio1/animais_ordenados.txt) |
<br>

![EvidÃªncia 2](./ExercÃ­cios/ExercÃ­cio1/EvidÃªncias/Evidencia2.png)

Etapa 3 â€“ Para validar a lÃ³gica antes de rodar a versÃ£o completa da terceira etapa, iniciei com uma versÃ£o reduzida no Jupyter Notebook. Instalei a biblioteca names atravÃ©s do terminal com o comando pip install names, e em seguida utilizei essa biblioteca para gerar 1.000 nomes Ãºnicos. A partir dessa base, gerei uma lista com 10 mil nomes aleatÃ³rios usando random.choice() e salvei os dados em um arquivo chamado nomes_aleatorios_teste.txt, com um nome por linha. Esse teste me permitiu garantir que toda a lÃ³gica de geraÃ§Ã£o e gravaÃ§Ã£o estava funcionando corretamente, antes de executar o processo completo no terminal.

| Arquivo | Link |
|--------|------|
| nomes_aleatorios_teste.txt | [ğŸ”— nomes_aleatorios_teste.txt](./ExercÃ­cios/ExercÃ­cio1/nomes_aleatorios_teste.txt) |
<br>

![EvidÃªncia 3](./ExercÃ­cios/ExercÃ­cio1/EvidÃªncias/Evidencia3.png)
<br>

ApÃ³s validar a lÃ³gica da geraÃ§Ã£o de nomes com a versÃ£o de teste no Jupyter Notebook, desenvolvi a versÃ£o final da terceira etapa em um script separado chamado Etapa3.py. Neste script, utilizei a biblioteca names para gerar 39.080 nomes Ãºnicos e, a partir dessa base, criei 10 milhÃµes de nomes aleatÃ³rios com a funÃ§Ã£o random.choice(). Para garantir performance na escrita, abri o arquivo nomes_aleatorios.txt uma Ãºnica vez utilizando o comando with open(), gravando todos os nomes de forma sequencial, um por linha. Executei esse script diretamente no terminal com o comando python Etapa3.py, o que permitiu melhor desempenho no processamento e na geraÃ§Ã£o do arquivo final. O arquivo nomes_aleatorios.txt nÃ£o estÃ¡ no repositÃ³rio por exceder o limite de tamanho do GitHub, mas que ele pode ser facilmente reproduzido executando o script Etapa3.py.

| Arquivo | Link |
|--------|------|
| Etapa3.py | [ğŸ”— Etapa3.py](./ExercÃ­cios/ExercÃ­cio1/Etapa3.py) |
<br>

![EvidÃªncia 4](./ExercÃ­cios/ExercÃ­cio1/EvidÃªncias/Evidencia4.png)

<br>

## 2.2 Apache Spark

Neste exercÃ­cio, o objetivo foi praticar a manipulaÃ§Ã£o de dados em larga escala utilizando o framework Apache Spark por meio da biblioteca PySpark. As atividades foram organizadas em etapas sequenciais, onde a cada passo novas transformaÃ§Ãµes e consultas foram realizadas sobre o DataFrame . Inicialmente, configurei o ambiente de execuÃ§Ã£o, garantindo a integraÃ§Ã£o correta entre o Python, o Java e o Spark, e criei um script de execuÃ§Ã£o automÃ¡tica *run_pyspark.ps1* para facilitar os testes no terminal do Windows. A leitura e a estruturaÃ§Ã£o dos dados foram feitas a partir do arquivo nomes_aleatorios.txt, e o DataFrame df_nomes foi enriquecido com colunas adicionais como Escolaridade, PaÃ­s, Ano de Nascimento e GeraÃ§Ã£o.

A seguir, descrevo cada etapa do processo, incluindo os cÃ³digos desenvolvidos:

| Arquivo | Link |
|--------|------|
| Exercicio2.py | [ğŸ”— Exercicio2.py](./ExercÃ­cios/ExercÃ­cio2/Exercicio2.py) |
| run_pyspark.ps1 | [ğŸ”— run_pyspark.ps1](./ExercÃ­cios/ExercÃ­cio2/run_pyspark.ps1) |
<br>

>ResoluÃ§Ã£o:

Etapa 1 â€“ Criei o script Etapa1.py onde importei as bibliotecas SparkSession, SparkContext e SQLContext. Em seguida, criei a sessÃ£o Spark, e utilizei spark.read.csv para carregar o arquivo nomes_aleatorios.txt, que estÃ¡ localizado na pasta do exercÃ­cio 1, o caminho foi ajustado corretamente para acessar o arquivo. Por fim, visualizei as 5 primeiras linhas com o mÃ©todo .show(5).

![EvidÃªncia 1](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia1.png)

Etapa 2 â€“ Criei o script Exercicio2.py, onde continuei utilizando a sessÃ£o Spark iniciada anteriormente. Inicialmente, carreguei novamente o arquivo nomes_aleatorios.txt usando o mesmo caminho relativo. Como o Spark nÃ£o reconheceu automaticamente os nomes das colunas, a coluna foi nomeada como _c0. Renomeei essa coluna para Nomes utilizando o mÃ©todo withColumnRenamed. Em seguida, utilizei o mÃ©todo printSchema() para visualizar a estrutura do dataframe, confirmando que a coluna agora se chama Nomes e estÃ¡ com o tipo string. Por fim, usei o mÃ©todo .show(10) para exibir as 10 primeiras linhas do dataframe, acessando a coluna pelo formato de Ã­ndice, conforme exigido no enunciado.

![EvidÃªncia 2](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia2.png)

Etapa 3 â€“ ApÃ³s carregar e renomear a coluna, adicionei uma nova coluna chamada Escolaridade. Para isso, criei uma coluna auxiliar rand_num com valores aleatÃ³rios inteiros de 0 a 2, usando a funÃ§Ã£o rand com semente fixa para reprodutibilidade e a funÃ§Ã£o floor. Depois, com o mÃ©todo when, atribuÃ­ os valores "Fundamental", "MÃ©dio" e "Superior" conforme o nÃºmero aleatÃ³rio gerado. Removi a coluna auxiliar rand_num em seguida. Para conferir o resultado, utilizei printSchema() para ver a estrutura do dataframe e select().show() para exibir as 10 primeiras linhas com os nomes e escolaridades.

AlÃ©m do script Python, criei o arquivo run_pyspark.ps1 para facilitar a execuÃ§Ã£o dos scripts PySpark, configurando a variÃ¡vel de ambiente JAVA_HOME com o JDK 8 antes de rodar o cÃ³digo. 

![EvidÃªncia 3](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia3.png)

Etapa 4 â€“ Nesta etapa, adicionei ao dataframe uma nova coluna chamada Pais, contendo valores aleatÃ³rios entre 13 paÃ­ses da AmÃ©rica do Sul. Para isso, criei uma coluna auxiliar chamada rand_pais, gerando nÃºmeros inteiros de 0 a 12 com a funÃ§Ã£o rand combinada com floor, e usando uma semente fixa. Em seguida, utilizei a funÃ§Ã£o expr do PySpark para construir uma expressÃ£o CASE WHEN que mapeia cada nÃºmero para um paÃ­s da lista. ApÃ³s o mapeamento, removi a coluna auxiliar rand_pais e finalizei exibindo o esquema do DataFrame com printSchema() e mostrando as 10 primeiras linhas com show().

![EvidÃªncia 4](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia4.png)

Etapa 5 â€“ Adicionei uma nova coluna chamada AnoNascimento ao dataframe, utilizei a funÃ§Ã£o rand com semente fixa e multipliquei por 66, que corresponde Ã  quantidade de anos entre 1945 e 2010. Em seguida, apliquei a funÃ§Ã£o floor para obter nÃºmeros inteiros e somei 1945, garantindo que os valores finais ficassem no intervalo desejado, depois atribuÃ­ esse resultado diretamente Ã  nova coluna utilizando withColumn.

![EvidÃªncia 5](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia5.png)

Etapa 6 â€“ Criei um novo dataframe chamado df_select contendo apenas as pessoas nascidas a partir de 2001, ou seja, neste sÃ©culo. Para isso, utilizei o mÃ©todo select para extrair as colunas "Nomes" e "AnoNascimento" do df_nomes, aplicando em seguida o mÃ©todo where com a condiÃ§Ã£o AnoNascimento >= 2001.  Por fim, utilizei o mÃ©todo show(10) para visualizar os 10 primeiros nomes resultantes desse novo dataframe.

![EvidÃªncia 6](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia6.png)

Etapa 7 â€“ Para repetir a seleÃ§Ã£o das pessoas que nasceram neste sÃ©culo usando Spark SQL, primeiro registrei o dataframe df_nomes como uma tabela temporÃ¡ria chamada pessoas com o mÃ©todo createOrReplaceTempView(). Em seguida, executei uma consulta SQL para selecionar os nomes e anos de nascimento onde o ano Ã© maior ou igual a 200, o resultado foi armazenado no dataframe df_select. Dessa forma, realizei a filtragem usando a linguagem SQL diretamente no Spark.

![EvidÃªncia 7](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia7.png)

Etapa 8 â€“ Nesta etapa, utilizei o mÃ©todo filter do DataFrame df_nomes para filtrar as pessoas que pertencem Ã  geraÃ§Ã£o Millennials. Para isso, apliquei uma condiÃ§Ã£o lÃ³gica com operadores >= e <= na coluna AnoNascimento. Em seguida, utilizei o mÃ©todo count() para contar quantas linhas atendem a essa condiÃ§Ã£o e imprimi o resultado no console.

![EvidÃªncia 8](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia8.png)

Etapa 9 â€“ Utilizei Spark SQL para contar quantas pessoas pertencem Ã  geraÃ§Ã£o Millennials, assim, usei novamente a temp view chamada pessoas, criada a partir do df_nomes. Em seguida, escrevi uma query SQL usando o comando SELECT COUNT(*) com a clÃ¡usula WHERE AnoNascimento BETWEEN 1980 AND 1994. O resultado foi exibido com o mÃ©todo .show(), apresentando a quantidade total de registros que atendem ao critÃ©rio da geraÃ§Ã£o Millennials.

![EvidÃªncia 9](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia9.png)

Etapa 10 â€“ Nesta etapa, utilizei Spark SQL para contar a quantidade de pessoas de cada paÃ­s pertencentes a diferentes geraÃ§Ãµes. Para isso, primeiro registrei o dataframe df_nomes como uma tabela temporÃ¡ria chamada pessoas. Em seguida, construÃ­ uma consulta SQL utilizando a clÃ¡usula CASE para categorizar cada pessoa em uma das quatro geraÃ§Ãµes dadas, com uma opÃ§Ã£o adicional chamada "Outros" para qualquer caso fora dessas faixas. A consulta agrupou os dados por paÃ­s e geraÃ§Ã£o e contou o nÃºmero de registros em cada combinaÃ§Ã£o. Por fim, ordenei o resultado por paÃ­s, geraÃ§Ã£o e quantidade, e exibi as linhas com o mÃ©todo .show().

![EvidÃªncia 10](./ExercÃ­cios/ExercÃ­cio2/EvidÃªncias/Evidencia10.png)

<br>

---


# Certificados


Durante essa sprint, concluÃ­ os cursos AWS: Fundamentals of Analytics on AWS â€“ Part 2 (PortuguÃªs) e AWS Glue Getting Started (PortuguÃªs). A seguir, compartilho os certificados correspondentes:

| Certificado | Link |
|--------|------|
| Fundamentals of Analytics on AWS â€“ Part 2 (PortuguÃªs) | [ğŸ”— Link ](./Certificados/Fundamentals%20of%20Analytics%20on%20AWS%20â€“%20Part%202%20(PortuguÃªs).pdf) |
| AWS Glue Getting Started (PortuguÃªs) | [ğŸ”— Link ](./Certificados/AWS%20Glue%20Getting%20Started%20(PortuguÃªs).pdf) |
<br>
