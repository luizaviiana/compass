{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aa96df",
   "metadata": {},
   "source": [
    "# Etapa 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408128e",
   "metadata": {},
   "source": [
    "Conhecendo um pouco mais do arquivo antes de fazer o script de teste para entender o seu formato e os dados contidos no arquivo nomes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a23593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>sexo</th>\n",
       "      <th>total</th>\n",
       "      <th>ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jennifer</td>\n",
       "      <td>F</td>\n",
       "      <td>54336</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jessica</td>\n",
       "      <td>F</td>\n",
       "      <td>45278</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amanda</td>\n",
       "      <td>F</td>\n",
       "      <td>33752</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashley</td>\n",
       "      <td>F</td>\n",
       "      <td>33292</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>F</td>\n",
       "      <td>27228</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nome sexo  total   ano\n",
       "0  Jennifer    F  54336  1983\n",
       "1   Jessica    F  45278  1983\n",
       "2    Amanda    F  33752  1983\n",
       "3    Ashley    F  33292  1983\n",
       "4     Sarah    F  27228  1983"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('nomes.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713505d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1825433 entries, 0 to 1825432\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   nome    object\n",
      " 1   sexo    object\n",
      " 2   total   int64 \n",
      " 3   ano     int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 55.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c7f4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825433, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b8313",
   "metadata": {},
   "source": [
    "Testando o codigo localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b569df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      "\n",
      "+-------+----+-----+----+\n",
      "|   nome|sexo|total| ano|\n",
      "+-------+----+-----+----+\n",
      "|   Mary|   F|56918|1934|\n",
      "|  Betty|   F|31079|1934|\n",
      "|Barbara|   F|29232|1934|\n",
      "|Shirley|   F|22836|1934|\n",
      "|Dorothy|   F|21280|1934|\n",
      "+-------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"\\\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestarFiltro1934\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "caminho_csv = \"nomes.csv\"\n",
    "caminho_saida = \"saida_parquet\"\n",
    "\n",
    "df = spark.read.csv(caminho_csv, header=True, sep=\",\", inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df_1934 = df.filter(df[\"ano\"] == 1934)\n",
    "\n",
    "df_1934.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616ab6c",
   "metadata": {},
   "source": [
    "Código utilizado na AWS: \n",
    "\n",
    "Inicialmente, configurei os contextos necessários, depois li o arquivo CSV armazenado no S3 como um DynamicFrame e o converti para DataFrame para facilitar a aplicação de filtros. Apliquei um filtro para manter apenas as linhas em que o valor da coluna ano fosse igual a 1934, após aplicar o filtro, converti o DataFrame novamente para DynamicFrame e escrevi o resultado no S3 em formato Parquet, utilizando os parâmetros definidos para origem e destino no próprio Glue Job, finalizei a execução com o job.commit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2935587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "\n",
    "df_dyf = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [args['S3_INPUT_PATH']]},\n",
    "    format=\"csv\",\n",
    "    format_options={\"withHeader\": True, \"separator\": \",\"}\n",
    ")\n",
    "\n",
    "df = df_dyf.toDF()\n",
    "\n",
    "df_filtrado = df.filter(df[\"ano\"] == 1934)\n",
    "\n",
    "df_final = DynamicFrame.fromDF(df_filtrado, glueContext, \"df_final\")\n",
    "\n",
    " glueContext.write_dynamic_frame.from_options(\n",
    "    frame=df_final,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": args['S3_TARGET_PATH']},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978b43d",
   "metadata": {},
   "source": [
    "# Etapa 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3f62f",
   "metadata": {},
   "source": [
    "Testando o código localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c5078",
   "metadata": {},
   "source": [
    " iniciei o processamento local no Jupyter Notebook criando uma sessão Spark com as configurações do Java, depois realizei a leitura do arquivo nomes.csv, utilizando inferência automática de schema. Após verificar a estrutura com o método .printSchema(), apliquei uma transformação na coluna nome, convertendo todos os valores para letras maiúsculas usando a função upper(). Por fim, visualizei as 5 primeiras linhas para validar a leitura e transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879bb4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      "\n",
      "+--------+----+-----+----+\n",
      "|    nome|sexo|total| ano|\n",
      "+--------+----+-----+----+\n",
      "|JENNIFER|   F|54336|1983|\n",
      "| JESSICA|   F|45278|1983|\n",
      "|  AMANDA|   F|33752|1983|\n",
      "|  ASHLEY|   F|33292|1983|\n",
      "|   SARAH|   F|27228|1983|\n",
      "+--------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import upper, col\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"\\\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Etapa7_Parte1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "caminho_csv = \"nomes.csv\"\n",
    "\n",
    "df = spark.read.csv(caminho_csv, header=True, sep=\",\", inferSchema=True)\n",
    "\n",
    "df = df.withColumn(\"nome\", upper(col(\"nome\")))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692fbb86",
   "metadata": {},
   "source": [
    "Para fazer a contagem do total de linhas presentes realizei o agrupamento por ano e sexo, somando o total de registros para cada grupo, o resultado foi ordenado pelo ano de forma decrescente, mostrando primeiro os dados mais recentes e finalizei exibindo as primeiras linhas do agrupamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09fb2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas no dataframe: 1825433\n",
      "+----+----+---------------+\n",
      "| ano|sexo|total_registros|\n",
      "+----+----+---------------+\n",
      "|2014|   M|        1901376|\n",
      "|2014|   F|        1768775|\n",
      "|2013|   F|        1745339|\n",
      "|2013|   M|        1881463|\n",
      "|2012|   F|        1753922|\n",
      "|2012|   M|        1889414|\n",
      "|2011|   F|        1753500|\n",
      "|2011|   M|        1893230|\n",
      "|2010|   M|        1913851|\n",
      "|2010|   F|        1772738|\n",
      "|2009|   M|        1979303|\n",
      "|2009|   F|        1832925|\n",
      "|2008|   M|        2036289|\n",
      "|2008|   F|        1887234|\n",
      "|2007|   F|        1919408|\n",
      "|2007|   M|        2072139|\n",
      "|2006|   M|        2052377|\n",
      "|2006|   F|        1898463|\n",
      "|2005|   F|        1845379|\n",
      "|2005|   M|        1994841|\n",
      "+----+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_linhas = df.count()\n",
    "print(f\"Total de linhas no dataframe: {total_linhas}\")\n",
    "\n",
    "df_agrupado = df.groupBy(\"ano\", \"sexo\").sum(\"total\")\n",
    "df_agrupado = df_agrupado.withColumnRenamed(\"sum(total)\", \"total_registros\")\n",
    "df_agrupado = df_agrupado.orderBy(col(\"ano\").desc())\n",
    "\n",
    "df_agrupado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369478c7",
   "metadata": {},
   "source": [
    "Utilizei a função Window para particionar os dados por sexo e ordenar pelo total de registros em ordem decrescente, apliquei a função row_number para numerar os registros dentro de cada grupo e filtrei para obter o nome com maior registro para cada sexo,assim, pude indentificar o nome feminino e masculino mais popular, juntamente com o ano em que ocorreram seus maiores registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e782e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+\n",
      "|sexo| nome| ano|total|\n",
      "+----+-----+----+-----+\n",
      "|   F|LINDA|1947|99680|\n",
      "|   M|JAMES|1947|94755|\n",
      "+----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"sexo\").orderBy(desc(\"total\"))\n",
    "df_with_rank = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "top_names = df_with_rank.filter(col(\"rank\") == 1).select(\"sexo\", \"nome\", \"ano\", \"total\")\n",
    "\n",
    "top_names.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1faff",
   "metadata": {},
   "source": [
    "Realizei o agrupamento dos dados pela coluna ano, somando os valores da coluna total para obter a quantidade total de registros  por ano, oresultado foi ordenado de forma crescente e exibi as 10 primeiras linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9896efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "| ano|total_registros|\n",
      "+----+---------------+\n",
      "|1880|         201484|\n",
      "|1881|         192699|\n",
      "|1882|         221538|\n",
      "|1883|         216950|\n",
      "|1884|         243467|\n",
      "|1885|         240855|\n",
      "|1886|         255319|\n",
      "|1887|         247396|\n",
      "|1888|         299480|\n",
      "|1889|         288950|\n",
      "+----+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "total_por_ano = df.groupBy(\"ano\").agg(_sum(\"total\").alias(\"total_registros\"))\n",
    "total_por_ano.orderBy(\"ano\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b1918",
   "metadata": {},
   "source": [
    "Código utilizado na AWS: \n",
    "\n",
    "No script do AWS Glue, iniciei o job capturando os argumentos de entrada para identificar os caminhos no S3 e o nome do job, criei os contextos necessários  e realizei a leitura do arquivo nomes.csv no S3 como DynamicFrame, convertendo-o para DataFrame para aplicar as transformações. Verifiquei o schema e converti os nomes para letras maiúsculas com a função upper(), depois contei o total de registros e agrupei os dados por ano e sexo, somando os totais e ordenando os anos de forma decrescente. Utilizei uma janela de partição para identificar o nome mais frequente por sexo e ano, após isso, agrupei os dados novamente por ano, somando os totais e ordenando os 10 primeiros anos de forma crescente. Por fim, converti o DataFrame final de volta para DynamicFrame e escrevi os dados no S3 no formato JSON, particionando pelas colunas sexo e ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18437bde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'awsglue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mawsglue\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mawsglue\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getResolvedOptions\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'awsglue'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import col, upper, sum as _sum, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "df_dynamic = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [args['S3_INPUT_PATH']]},\n",
    "    format=\"csv\",\n",
    "    format_options={\"withHeader\": True, \"separator\": \",\"}\n",
    ")\n",
    "\n",
    "df = df_dynamic.toDF()\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df_upper = df.withColumn(\"nome\", upper(col(\"nome\")))\n",
    "\n",
    "print(\"Total de linhas no dataframe:\", df_upper.count())\n",
    "\n",
    "df_grouped = df_upper.groupBy(\"ano\", \"sexo\").agg(_sum(\"total\").alias(\"total_registros\"))\n",
    "df_grouped.orderBy(col(\"ano\").desc()).show()\n",
    "\n",
    "window_spec = Window.partitionBy(\"sexo\").orderBy(col(\"total\").desc())\n",
    "df_ranked = df_upper.withColumn(\"rank\", row_number().over(window_spec))\n",
    "df_ranked.filter(col(\"rank\") == 1).select(\"sexo\", \"nome\", \"ano\", \"total\").show()\n",
    "\n",
    "df_total_ano = df_upper.groupBy(\"ano\").agg(_sum(\"total\").alias(\"total_registros\"))\n",
    "df_total_ano.orderBy(\"ano\").show(10)\n",
    "\n",
    "df_final = DynamicFrame.fromDF(df_upper, glueContext, \"df_final\")\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=df_final,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\n",
    "        \"path\": args['S3_TARGET_PATH'],\n",
    "        \"partitionKeys\": [\"sexo\", \"ano\"]\n",
    "    },\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58122a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
