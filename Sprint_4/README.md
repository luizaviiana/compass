# üöÄ Sprint 4

## üìå Resumo

Durante a Sprint 4, iniciei meus estudos em cloud computing com foco na AWS. Atrav√©s dos cursos propostos, tive contato com os principais servi√ßos da plataforma sob diferentes perspectivas. Na segunda semana, reforcei meu aprendizado por meio dos exerc√≠cios pr√°ticos no Cloud Quest, al√©m dos exerc√≠cios propostos e do desafio da PB. Foi meu primeiro contato com a nuvem, a experi√™ncia foi bastante positiva e enriquecedora. A seguir, apresento um resumo dos principais t√≥picos abordados:

- **Data & Analytics**: Com base nas orienta√ß√µes iniciais, consegui realizar a configura√ß√£o da minha conta na AWS e pude seguir as orienta√ß√µes para realizar os exerc√≠cios e o desafio.

- **AWS Skill Builder - AWS Partner: Sales Accreditation (Business) (Portuguese)**: fui introduzida aos principais conceitos e benef√≠cios da computa√ß√£o em nuvem, com foco na proposta de valor da AWS para os clientes. O treinamento destacou como identificar oportunidades de neg√≥cios, alinhar as solu√ß√µes da AWS √†s necessidades espec√≠ficas dos clientes e articular os benef√≠cios estrat√©gicos da nuvem. Tamb√©m aprofundei meu entendimento sobre o modelo de responsabilidade compartilhada, os principais servi√ßos AWS e as pr√°ticas recomendadas para conduzir conversas de vendas eficazes, especialmente no contexto de transforma√ß√£o digital e migra√ß√£o para a nuvem.

- **AWS Partner: Economias na nuvem AWS**:  aprofundei meus conhecimentos sobre como a nuvem AWS pode gerar valor financeiro para os clientes. Aprendi a identificar e comunicar os principais benef√≠cios econ√¥micos da ado√ß√£o da nuvem, como otimiza√ß√£o de custos, escalabilidade e modelo de pagamento conforme o uso. O treinamento abordou conceitos como TCO, ROI e os mecanismos que a AWS oferece para controle e visibilidade dos gastos. 

- **AWS Skill Builder - AWS Cloud Quest: Cloud Practitioner**: desenvolvi conhecimentos fundamentais sobre computa√ß√£o em nuvem por meio de uma abordagem interativa e gamificada. Ao longo da jornada, assumi desafios pr√°ticos em um ambiente virtual, aplicando conceitos essenciais da AWS, como servi√ßos de computa√ß√£o, armazenamento, banco de dados, redes e seguran√ßa. O curso tamb√©m abordou o modelo de responsabilidade compartilhada, precifica√ß√£o e melhores pr√°ticas de arquitetura. 


ü§î *Reflex√µes*

A Sprint 4 foi especialmente desafiadora para mim, j√° que foi meu primeiro contato a AWS. No in√≠cio, enfrentei algumas dificuldades na configura√ß√£o da inst√¢ncia, mas consegui super√°-las com o apoio do meu squad. √Ä medida que fui avan√ßando nos exerc√≠cios, fui ganhando mais confian√ßa, e concluir o desafio final foi bastante gratificante. Al√©m disso, me sinto muito grata por todo o suporte que venho recebendo dos monitores, da Scrum Master, do meu time e dos demais membros da PB.


<br>

---

## üóÇÔ∏è Sum√°rio 

1. [Desafio](#desafio)
2. [Exerc√≠cios](#exerc√≠cios)
    - 2.1 [Lab AWS S3](#21-lab-aws-s3)
    - 2.2 [Lab AWS Athena](#22-lab-aws-athena)
    - 2.3 [Lab AWS Lambda](#23-lab-aws-lambda)


3. [Certificados](#certificados)

<br>

---

# [Desafio](./Desafio/) 

Nesse desafio da Sprint 3, o objetivo foi colocar em pr√°tica os conceitos aprendidos dos servi√ßos da AWS e an√°lise de dados, integrando conhecimentos adquiridos durante o PB e aprofundados em estudos complementares. 

Os arquivos utilizados para a realiza√ß√£o do desafio est√£o organizados em pastas por etapas, acompanhando as fases do desenvolvimento, e podem ser encontrados na *Pasta Desafio*. As evid√™ncias do processo est√£o armazenadas na *Pasta Evid√™ncias*. Para um detalhamento completo do desafio, recomendo consultar o README da pasta *Readme Desafio*, nele est√£o os arquivos no formato Jupyter Notebook para execu√ß√£o de cada etapa, os arquivos CSV que foram utilizados e os arquivos resultantes das an√°lises. Seguem os links:

- [Pasta Desafio](./Desafio/) 
- [Pasta Evid√™ncias](./Evid√™ncias/)
- [Readme Desafio](./Desafio/README.md)

Esse desafio foi desafiador e enriquecedor, principalmente pela gama de aprendizado que pude absorver com a AWS. Contar com a ajuda dos colegas da Squad e da monitoria e da Scrum Master foi fundamental para a conclus√£o.

<br>

---

# [Exerc√≠cios](./Exerc√≠cios/)

## 2.1 Lab AWS S3

Nesse desafio o objetivo √© explorar as capacidades do servi√ßo AWS S3.  Nos passos que foram fornecidos eu fui guiada pelas configura√ß√µes necess√°rias para que um bucket do Amazon S3 funcionasse como hospedagem de conte√∫do est√°tico. A seguir estarei compartilhando os arquivos utilizados e detalhando cada etapa que segui:

| Arquivo | Link |
|--------|------|
| nomes.csv | [üîó nomes.csv](./Exerc√≠cios/Exerc√≠cio1/nomes.csv) |
| nomes.ipynb | [üîó nomes.ipynb](./Exerc√≠cios/Exerc√≠cio1/nomes.ipynb) |
| index.html | [üîó nomes.index.html](./Exerc√≠cios/Exerc√≠cio1/index.html) |
| 404.html | [üîó 404.html](./Exerc√≠cios/Exerc√≠cio1/404.html) |


>Resolu√ß√£o:

**Configura√ß√£o da conta**

Inicialmente finalizei a configura√ß√£o da minha conta, como orientado no data analytics. Comecei essa etapa acessando o console da AWS e navegando at√© o servi√ßo EC2, com o objetivo de iniciar uma inst√¢ncia compat√≠vel com o Free Tier para fins de teste e aprendizado.

Durante a cria√ß√£o da inst√¢ncia, selecionei a Amazon Machine Image (AMI) padr√£o, Amazon Linux 2, e escolhi o tipo de inst√¢ncia t2.micro, que √© eleg√≠vel para o n√≠vel gratuito. Quando solicitado sobre o par de chaves de acesso, optei por ‚ÄúProceed without key pair‚Äù, j√° que o objetivo da atividade n√£o exigia conex√£o via SSH.

Na se√ß√£o de tags, adicionei as tr√™s tags obrigat√≥rias √† inst√¢ncia: Name, com o valor instancia-teste; CostCenter, com o valor Data & Analytics; e Project, com o valor Programa de Bolsas. Essas mesmas tags tamb√©m foram adicionadas ao volume no momento da cria√ß√£o. Segue a execu√ß√£o da inst√¢ncia:

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia1.png)


Ap√≥s iniciar a inst√¢ncia e deix√°-la rodando por alguns minutos, acessei o menu Volumes dentro do EC2 para localizar o volume EBS associado. Com isso, confirmei que as tags haviam sido corretamente aplicadas ao volume. Por fim, retornei √† inst√¢ncia e a encerrei utilizando a op√ß√£o ‚ÄúTerminate instance‚Äù, conforme instru√≠do na atividade.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia2.png)

Inst√¢ncia encerrada:

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia3.png)


**Exerc√≠cio Lab AWS S3**

Comecei a primeira etapa acessando o console da AWS e navegando at√© o servi√ßo S3, com o objetivo de explorar o recurso de hospedagem de sites est√°ticos.

Criei um bucket com o nome ana-luiza-static-site, selecionando a regi√£o US East (N. Virginia) e mantendo as demais configura√ß√µes padr√£o. Em seguida, ativei a funcionalidade de Static Website Hosting, informando o arquivo index.html como documento de √≠ndice e 404.html como documento de erro.

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia4.png)


Em seguida, ativei a funcionalidade de Static Website Hosting, informando o arquivo index.html como documento de √≠ndice e 404.html como documento de erro.

![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia5.png)

Para permitir o acesso p√∫blico ao conte√∫do do site, desabilitei o bloqueio de acesso p√∫blico do bucket e adicionei uma pol√≠tica de bucket que concede permiss√£o de leitura p√∫blica para qualquer usu√°rio da internet.

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia6.png)

![Evid√™ncia 7](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia7.png)

Na etapa seguinte, criei localmente o arquivo index.html, contendo uma mensagem de boas-vindas e um link para download do arquivo CSV. Tamb√©m criei um arquivo 404.html personalizado para exibir mensagens de erro em caso de p√°ginas inexistentes. Ambos os arquivos foram enviados ao bucket, garantindo que os nomes correspondessem exatamente aos definidos na configura√ß√£o da hospedagem.

![Evid√™ncia 8](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia8.png)


Por fim, testei o funcionamento do site acessando o endpoint do bucket, onde o conte√∫do foi carregado corretamente, com os acentos exibidos de forma adequada ap√≥s a inclus√£o da codifica√ß√£o UTF-8 no HTML.

http://ana-luiza-static-site.s3-website-us-east-1.amazonaws.com

![Evid√™ncia 9](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia9.png)

![Evid√™ncia 10](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia10.png)

<br>

## 2.2 Lab AWS Athena

Nesse desafio o objetivo √© explorar as capacidades do servi√ßo AWS S3. A seguir estarei compartilhando os arquivos utilizados e detalhando cada etapa que segui:

| Arquivo | Link |
|--------|------|
| consulta.sql | [üîó consulta.sql](./Exerc√≠cios/Exerc√≠cio2/consulta.sql) |
| nomes.ipynb | [üîó nomes.ipynb](./Exerc√≠cios/Exerc√≠cio1/nomes.ipynb) |

>Resolu√ß√£o:

**Lab AWS Athena**

No exerc√≠cio anterior j√° havia feito o download do arquivo nomes.csv, entao inicialmente fiz a an√°lise do arquivo conhecendo as colunas, suas dimens√µes e as principais informa√ß√µes.

Ap√≥s isso, acessei o AWS Athena para criar um banco de dados chamado meubanco, com o objetivo de organizar os dados que seriam analisados a partir do arquivo CSV armazenado no bucket S3.

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia1.png)

Em seguida, criei uma tabela externa no banco de dados, definindo os campos conforme os tipos de dados identificados no arquivo (nome, sexo, total e ano), e configurei a localiza√ß√£o do arquivo CSV no bucket S3 da regi√£o us-east-1.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia2.png)

Antes de executar as consultas, precisei criar a tabela nomes dentro do banco de dados meubanco, usando uma instru√ß√£o CREATE EXTERNAL TABLE. O objetivo foi permitir que o Athena consultasse diretamente os dados armazenados em um arquivo CSV no S3, sem a necessidade de importa√ß√£o para um banco tradicional.

Defini os campos da tabela conforme a estrutura do arquivo nomes.csv: nome e sexo como STRING, e total e ano como INT, respeitando os tipos identificados previamente no VS Code.

Utilizei o formato SERDE do tipo LazySimpleSerDe, que √© adequado para arquivos CSV e permite valores nulos. Especifiquei nas propriedades que o delimitador de campos √© uma v√≠rgula (',') e adicionei a instru√ß√£o TBLPROPERTIES ('skip.header.line.count' = '1') para que a primeira linha (cabe√ßalho) do CSV fosse ignorada.

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia3.png)

Apontei o LOCATION para 's3://ana-luiza-static-site/dados/', que √© a pasta onde o arquivo nomes.csv est√° armazenado. Isso permite ao Athena mapear corretamente os dados no S3 para a tabela criada.

Por fim, elaborei uma consulta SQL que lista os 3 nomes mais usados em cada d√©cada a partir de 1950, utilizando fun√ß√µes de agrega√ß√£o e a cl√°usula ROW_NUMBER() para ordenar e filtrar os nomes mais populares por d√©cada. Essa consulta foi estruturada em uma CTE para garantir a correta aplica√ß√£o dos filtros e ordena√ß√µes. 

Primeiro c√≥digo: 

Comecei testando os dados com uma consulta simples para verificar se a tabela havia sido criada corretamente e se os dados estavam sendo lidos pelo Athena.
Essa consulta retorna os 15 nomes mais utilizados no ano de 1999, ordenando pela coluna total em ordem decrescente. O objetivo foi validar o conte√∫do da tabela e entender melhor a distribui√ß√£o dos dados.

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia4.png)

![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia5.png)


Segundo c√≥digo:

Em seguida, desenvolvi uma consulta mais elaborada com o objetivo de identificar os tr√™s nomes mais populares em cada d√©cada, a partir de 1950.

Para isso, utilizei uma CTE (Common Table Expression) nomeada como ranked_names. Dentro dela, primeiro agrupei os dados por nome e d√©cada, somando a quantidade total de ocorr√™ncias de cada nome naquele intervalo. A d√©cada foi calculada utilizando FLOOR(ano / 10) * 10.

Depois, apliquei a fun√ß√£o ROW_NUMBER() com PARTITION BY decada e ORDER BY total DESC para numerar os nomes em ordem de popularidade dentro de cada d√©cada.

Na consulta final, selecionei apenas os registros com posicao <= 3 para garantir que fossem retornados apenas os tr√™s nomes mais utilizados em cada d√©cada.

A ordena√ß√£o final foi feita por decada e posicao, organizando os resultados de forma cronol√≥gica e por ranking.

![Evid√™ncia 7](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia7.png)

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia6.png)


<br>

--- 



## 2.3 Lab AWS Lambda

Nesse desafio o objetivo √© explorar as capacidades do servi√ßo AWS S3. A seguir estarei compartilhando os arquivos utilizados e detalhando cada etapa que segui:

| Arquivo | Link |
|--------|------|
| Dockerfile | [üîó Dockerfile](./Exerc√≠cios/Exerc√≠cio3/Dockerfile) |
| minha-camada-pandas.zip | [üîó minha-camada-pandas.zip](./Exerc√≠cios/Exerc√≠cio3/minha-camada-pandas.zip) |

>Resolu√ß√£o:

**Lab AWS Lambda**

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia1.png)



---


# Certificados


Durante essa sprint, conclu√≠ os cursos AWS Partner: Accreditation (Technical) (Portugu√™s) e AWS Technical Essentials. A seguir, compartilho os certificados correspondentes:

| Certificado | Link |
|--------|------|
|AWS Partner: Accreditation | [üîó Accreditation](./Certificados/Certificado%20AWS%20Partner%20Accreditation%20(Technical)%20(Portugu√™s).png) |
|AWS Technical Essentials | [üîó Essentials](./Certificados/Certificado%20AWS%20Technical%20Essentials.png) |
|AWS Cloud Quest Practitioner | [üîó Practitioner](https://www.credly.com/badges/e6de8a04-d292-415f-a306-bca96da7177f/public_url) |