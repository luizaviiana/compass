# üöÄ Sprint 4

## üìå Resumo

Durante a Sprint 4, iniciei meus estudos em cloud computing com foco na AWS. Atrav√©s dos cursos propostos, tive contato com os principais servi√ßos da plataforma sob diferentes perspectivas. Na segunda semana, reforcei meu aprendizado por meio dos exerc√≠cios pr√°ticos no Cloud Quest, al√©m dos exerc√≠cios propostos e do desafio da PB. Foi meu primeiro contato com a nuvem, a experi√™ncia foi bastante positiva e enriquecedora. A seguir, apresento um resumo dos principais t√≥picos abordados:

- **Data & Analytics**: Com base nas orienta√ß√µes iniciais, consegui realizar a configura√ß√£o da minha conta na AWS e pude seguir as orienta√ß√µes para realizar os exerc√≠cios e o desafio.

- **AWS Skill Builder - AWS Partner: Sales Accreditation (Business) (Portuguese)**: fui introduzida aos principais conceitos e benef√≠cios da computa√ß√£o em nuvem, com foco na proposta de valor da AWS para os clientes. O treinamento destacou como identificar oportunidades de neg√≥cios, alinhar as solu√ß√µes da AWS √†s necessidades espec√≠ficas dos clientes e articular os benef√≠cios estrat√©gicos da nuvem. Tamb√©m aprofundei meu entendimento sobre o modelo de responsabilidade compartilhada, os principais servi√ßos AWS e as pr√°ticas recomendadas para conduzir conversas de vendas eficazes, especialmente no contexto de transforma√ß√£o digital e migra√ß√£o para a nuvem.

- **AWS Partner: Economias na nuvem AWS**:  aprofundei meus conhecimentos sobre como a nuvem AWS pode gerar valor financeiro para os clientes. Aprendi a identificar e comunicar os principais benef√≠cios econ√¥micos da ado√ß√£o da nuvem, como otimiza√ß√£o de custos, escalabilidade e modelo de pagamento conforme o uso. O treinamento abordou conceitos como TCO, ROI e os mecanismos que a AWS oferece para controle e visibilidade dos gastos. 

- **AWS Skill Builder - AWS Cloud Quest: Cloud Practitioner**: desenvolvi conhecimentos fundamentais sobre computa√ß√£o em nuvem por meio de uma abordagem interativa e gamificada. Ao longo da jornada, assumi desafios pr√°ticos em um ambiente virtual, aplicando conceitos essenciais da AWS, como servi√ßos de computa√ß√£o, armazenamento, banco de dados, redes e seguran√ßa. O curso tamb√©m abordou o modelo de responsabilidade compartilhada, precifica√ß√£o e melhores pr√°ticas de arquitetura. 


ü§î *Reflex√µes*

A Sprint 4 foi especialmente desafiadora para mim, j√° que foi meu primeiro contato a AWS. No in√≠cio, enfrentei algumas dificuldades na configura√ß√£o da inst√¢ncia, mas consegui super√°-las com o apoio do meu squad. √Ä medida que fui avan√ßando nos exerc√≠cios, fui ganhando mais confian√ßa, e concluir o desafio final foi bastante gratificante. Al√©m disso, me sinto muito grata por todo o suporte que venho recebendo dos monitores, da Scrum Master, do meu time e dos demais membros da PB.


<br>

---

## üóÇÔ∏è Sum√°rio 

1. [Desafio](#desafio)
2. [Exerc√≠cios](#exerc√≠cios)
    - 2.1 [Lab AWS S3](#21-lab-aws-s3)
    - 2.2 [Lab AWS Athena](#22-lab-aws-athena)
    - 2.3 [Lab AWS Lambda](#23-lab-aws-lambda)
    - 2.4 [Lab AWS - Limpeza de recursos ](#24-lab-aws---limpeza-de-recursos)


3. [Certificados](#certificados)

<br>

---

# [Desafio](./Desafio/) 

Nesse desafio da Sprint 4, o objetivo foi colocar em pr√°tica os conceitos aprendidos dos servi√ßos da AWS e an√°lise de dados, integrando conhecimentos adquiridos durante o PB e aprofundados em estudos complementares. 

Os arquivos utilizados para a realiza√ß√£o do desafio est√£o organizados em pastas por etapas, acompanhando as fases do desenvolvimento, e podem ser encontrados na *Pasta Desafio*. As evid√™ncias do processo est√£o armazenadas na *Pasta Evid√™ncias*. Para um detalhamento completo do desafio, recomendo consultar o README da pasta *Readme Desafio*, nele est√£o os arquivos no formato Jupyter Notebook para execu√ß√£o de cada etapa, os arquivos CSV que foram utilizados e os arquivos resultantes das an√°lises. Seguem os links:

- [Pasta Desafio](./Desafio/) 
- [Pasta Evid√™ncias](./Evid√™ncias/)
- [Readme Desafio](./Desafio/README.md)

Esse desafio foi desafiador e enriquecedor, principalmente pela gama de aprendizado que pude absorver com a AWS. Contar com a ajuda dos colegas da Squad e da monitoria e da Scrum Master foi fundamental para a conclus√£o.

<br>

---

# [Exerc√≠cios](./Exerc√≠cios/)

## 2.1 Lab AWS S3

Nesse desafio o objetivo √© explorar as capacidades do servi√ßo AWS S3.  Nos passos que foram fornecidos eu fui guiada pelas configura√ß√µes necess√°rias para que um bucket do Amazon S3 funcionasse como hospedagem de conte√∫do est√°tico. A seguir estarei compartilhando os arquivos utilizados e detalhando cada etapa que segui:

| Arquivo | Link |
|--------|------|
| nomes.csv | [üîó nomes.csv](./Exerc√≠cios/Exerc√≠cio1/nomes.csv) |
| nomes.ipynb | [üîó nomes.ipynb](./Exerc√≠cios/Exerc√≠cio1/nomes.ipynb) |
| index.html | [üîó nomes.index.html](./Exerc√≠cios/Exerc√≠cio1/index.html) |
| 404.html | [üîó 404.html](./Exerc√≠cios/Exerc√≠cio1/404.html) |


>Resolu√ß√£o:

**Configura√ß√£o da conta**

Inicialmente finalizei a configura√ß√£o da minha conta, como orientado no data analytics. Comecei essa etapa acessando o console da AWS e navegando at√© o servi√ßo EC2, com o objetivo de iniciar uma inst√¢ncia compat√≠vel com o Free Tier para fins de teste e aprendizado.

Durante a cria√ß√£o da inst√¢ncia, selecionei a Amazon Machine Image (AMI) padr√£o, Amazon Linux 2, e escolhi o tipo de inst√¢ncia t2.micro, que √© eleg√≠vel para o n√≠vel gratuito. Quando solicitado sobre o par de chaves de acesso, optei por ‚ÄúProceed without key pair‚Äù, j√° que o objetivo da atividade n√£o exigia conex√£o via SSH.

Na se√ß√£o de tags, adicionei as tr√™s tags obrigat√≥rias √† inst√¢ncia: Name, com o valor instancia-teste; CostCenter, com o valor Data & Analytics; e Project, com o valor Programa de Bolsas. Essas mesmas tags tamb√©m foram adicionadas ao volume no momento da cria√ß√£o. Segue a execu√ß√£o da inst√¢ncia:

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia1.png)


Ap√≥s iniciar a inst√¢ncia e deix√°-la rodando por alguns minutos, acessei o menu Volumes dentro do EC2 para localizar o volume EBS associado. Com isso, confirmei que as tags haviam sido corretamente aplicadas ao volume. Por fim, retornei √† inst√¢ncia e a encerrei utilizando a op√ß√£o ‚ÄúTerminate instance‚Äù, conforme instru√≠do na atividade.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia2.png)

Inst√¢ncia encerrada:

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia3.png)


**Exerc√≠cio Lab AWS S3**

Comecei a primeira etapa acessando o console da AWS e navegando at√© o servi√ßo S3, com o objetivo de explorar o recurso de hospedagem de sites est√°ticos.

Criei um bucket com o nome ana-luiza-static-site, selecionando a regi√£o US East (N. Virginia) e mantendo as demais configura√ß√µes padr√£o. Em seguida, ativei a funcionalidade de Static Website Hosting, informando o arquivo index.html como documento de √≠ndice e 404.html como documento de erro.

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia4.png)


Em seguida, ativei a funcionalidade de Static Website Hosting, informando o arquivo index.html como documento de √≠ndice e 404.html como documento de erro.

![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia5.png)

Para permitir o acesso p√∫blico ao conte√∫do do site, desabilitei o bloqueio de acesso p√∫blico do bucket e adicionei uma pol√≠tica de bucket que concede permiss√£o de leitura p√∫blica para qualquer usu√°rio da internet.

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia6.png)

![Evid√™ncia 7](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia7.png)

Na etapa seguinte, criei localmente o arquivo index.html, contendo uma mensagem de boas-vindas e um link para download do arquivo CSV. Tamb√©m criei um arquivo 404.html personalizado para exibir mensagens de erro em caso de p√°ginas inexistentes. Ambos os arquivos foram enviados ao bucket, garantindo que os nomes correspondessem exatamente aos definidos na configura√ß√£o da hospedagem.

![Evid√™ncia 8](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia8.png)


Por fim, testei o funcionamento do site acessando o endpoint do bucket, onde o conte√∫do foi carregado corretamente, com os acentos exibidos de forma adequada ap√≥s a inclus√£o da codifica√ß√£o UTF-8 no HTML.

http://ana-luiza-static-site.s3-website-us-east-1.amazonaws.com

![Evid√™ncia 9](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia9.png)

![Evid√™ncia 10](./Exerc√≠cios/Exerc√≠cio1/Evid√™ncias/Evidencia10.png)

<br>

## 2.2 Lab AWS Athena

Nesse desafio o objetivo √© explorar as capacidades do servi√ßo AWS S3. A seguir estarei compartilhando os arquivos utilizados e detalhando cada etapa que segui:

| Arquivo | Link |
|--------|------|
| consulta.sql | [üîó consulta.sql](./Exerc√≠cios/Exerc√≠cio2/consulta.sql) |
| nomes.ipynb | [üîó nomes.ipynb](./Exerc√≠cios/Exerc√≠cio1/nomes.ipynb) |

>Resolu√ß√£o:

**Lab AWS Athena**

No exerc√≠cio anterior j√° havia feito o download do arquivo nomes.csv, entao inicialmente fiz a an√°lise do arquivo conhecendo as colunas, suas dimens√µes e as principais informa√ß√µes.

Ap√≥s isso, acessei o AWS Athena para criar um banco de dados chamado meubanco, com o objetivo de organizar os dados que seriam analisados a partir do arquivo CSV armazenado no bucket S3.

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia1.png)

Em seguida, criei uma tabela externa no banco de dados, definindo os campos conforme os tipos de dados identificados no arquivo (nome, sexo, total e ano), e configurei a localiza√ß√£o do arquivo CSV no bucket S3 da regi√£o us-east-1.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia2.png)

Antes de executar as consultas, precisei criar a tabela nomes dentro do banco de dados meubanco, usando uma instru√ß√£o CREATE EXTERNAL TABLE. O objetivo foi permitir que o Athena consultasse diretamente os dados armazenados do um arquivo csv no S3, sem a necessidade de importa√ß√£o para um banco tradicional. Defini os campos da tabela conforme a estrutura do arquivo nomes.csv: nome e sexo como STRING, e total e ano como INT, respeitando os tipos identificados previamente no vscode.

Utilizei o formato SERDE do tipo LazySimpleSerDe, que √© adequado para arquivos CSV e permite valores nulos. Especifiquei nas propriedades que o delimitador de campos √© uma v√≠rgula  e adicionei a instru√ß√£o TBLPROPERTIES  para que a primeira linha do CSV fosse ignorada.

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia3.png)

Apontei o LOCATION para 's3://ana-luiza-static-site/dados/', que √© a pasta onde o arquivo nomes.csv est√° armazenado. Isso permite ao Athena mapear corretamente os dados no S3 para a tabela criada.

Por fim, elaborei uma consulta SQL que lista os 3 nomes mais usados em cada d√©cada a partir de 1950, utilizando fun√ß√µes de agrega√ß√£o para ordenar e filtrar os nomes mais populares por d√©cada. Essa consulta foi estruturada em uma CTE para garantir a correta aplica√ß√£o dos filtros e ordena√ß√µes. 

Primeiro c√≥digo: 

Comecei testando os dados com uma consulta simples para verificar se a tabela havia sido criada corretamente e se os dados estavam sendo lidos pelo Athena.
Essa consulta retorna os 15 nomes mais utilizados no ano de 1999, ordenando pela coluna total em ordem decrescente. O objetivo foi validar o conte√∫do da tabela e entender melhor a distribui√ß√£o dos dados.

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia4.png)

![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia5.png)


Segundo c√≥digo:

Em seguida, desenvolvi uma consulta mais elaborada com o objetivo de identificar os tr√™s nomes mais populares em cada d√©cada, a partir de 1950.

Para isso, utilizei uma CTE nomeada como ranked_names. Dentro dela, primeiro agrupei os dados por nome e d√©cada, somando a quantidade total de ocorr√™ncias de cada nome naquele intervalo. A d√©cada foi calculada utilizando FLOOR(ano / 10) * 10. Depois, apliquei a fun√ß√£o ROW_NUMBER() com PARTITION BY decada e ORDER BY total DESC para numerar os nomes em ordem de popularidade dentro de cada d√©cada.

Na consulta final, selecionei apenas os registros com posicao <= 3 para garantir que fossem retornados apenas os tr√™s nomes mais utilizados em cada d√©cada.

A ordena√ß√£o final foi feita por decada e posicao, organizando os resultados de forma cronol√≥gica e por ranking.

![Evid√™ncia 7](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia7.png)

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio2/Evid√™ncias/Evidencia6.png)


<br>

--- 



## 2.3 Lab AWS Lambda

Nesse desafio o objetivo √© explorar as capacidades do servi√ßo AWS S3. A seguir estarei compartilhando os arquivos utilizados e detalhando cada etapa que segui:

| Arquivo | Link |
|--------|------|
| Dockerfile | [üîó Dockerfile](./Exerc√≠cios/Exerc√≠cio3/Dockerfile) |
| minha-camada-pandas.zip | [üîó minha-camada-pandas.zip](./Exerc√≠cios/Exerc√≠cio3/minha-camada-pandas.zip) |

>Resolu√ß√£o:

**Lab AWS Lambda**

No console do AWS Lambda, selecionei criar uma fun√ß√£o, onde comecei uma do zero, defini o nome como MyLambdaFunction, em runtime, escolhi Python 3.9 e a arquitetura x86_64. Cliquei em ‚ÄúCriar fun√ß√£o‚Äù, e ap√≥s alguns instantes, a fun√ß√£o foi criada com sucesso e fui redirecionada para o editor de c√≥digo do Lambda.

![Evid√™ncia 1](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia1.png)

O console da AWS cria automaticamente um arquivo chamado lambda_function.py com um c√≥digo inicial, ent√£o, substitu√≠ esse c√≥digo pelo script, que acessa o arquivo CSV no S3 e utiliza a biblioteca Numpy e Pandas para realizar a opera√ß√£o. Cliquei em Deploy para que ocorresse a altera√ß√£o do c√≥digo e realizei o teste da Lambda clicando em Test e escolhendo o nome testeS3.

![Evid√™ncia 2](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia2.png)

Ao executar o teste ocorreu um erro pois o servi√ßo AWS Lambda n√£o possui a biblioteca pandas. Precisei criar uma layer para importar estas bibliotecas necess√°rias a Lambda.

![Evid√™ncia 3](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia3.png)

Assim, foi necess√°rio preparar uma Layer personalizada contendo a biblioteca pandas, j√° que ela n√£o est√° dispon√≠vel por padr√£o no ambiente da AWS Lambda. Para garantir compatibilidade, optou-se por criar essa camada usando um container Docker com o sistema operacional Amazon Linux 2023. O processo come√ßou com a cria√ß√£o de uma pasta e um arquivo Dockerfile, que define a imagem base e os comandos para instalar o Python 3.9, o gerenciador de pacotes pip e o utilit√°rio zip. Esses componentes s√£o necess√°rios para instalar as bibliotecas e compactar os arquivos.

Em seguida, foi constru√≠da uma imagem Docker com esse Dockerfile, permitindo a cria√ß√£o de um ambiente isolado e compat√≠vel, no qual as bibliotecas necess√°rias podem ser instaladas corretamente para posterior uso como layer na fun√ß√£o Lambda.

![Evid√™ncia 4](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia4.png)


Ap√≥s criar a imagem Docker, acessei o shell do container para configurar a estrutura da layer. Dentro do container, criei a pasta layer_dir/python, onde instalei a biblioteca pandas com o comando pip3 install -t ..

Em seguida, compactei a pasta python em um arquivo minha-camada-pandas.zip e copiei o .zip do container para a m√°quina local usando docker cp. Como o arquivo pode ultrapassar 10 MB, fiz o upload para o bucket S3, seguindo a recomenda√ß√£o da AWS para layers maiores.

![Evid√™ncia 5](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia5.png)

![Evid√™ncia 6](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia6.png)

Na etapa, acessei o console do AWS Lambda e criei uma nova camada chamada PandasLayer. Selecinoei a op√ß√£o de upload via URL do S3 e coloquei o link do arquivo minha-camada-pandas.zip previamente enviado. Defini a arquitetura como x86_64 e o tempo de execu√ß√£o como Python 3.9. Por fim, foi feita a cria√ß√£o da camada.

![Evid√™ncia 7](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia7.png)

 Ap√≥s isso, vinculei a camada criada √† fun√ß√£o Lambda, em que acessei a fun√ß√£o no console, abri a se√ß√£o de camadas e adicionei a PandasLayer personalizada criada anteriormente. 

![Evid√™ncia 8](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia8.png)

Executando o teste configurado anteriormente, o retorno esperado indicou que era recomendado aumentar o tempo limite e a mem√≥ria da fun√ß√£o.

![Evid√™ncia 9](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia9.png)

Assim, segui essa recomenda√ß√£o e aumentei tanto o tempo limite quanto a mem√≥ria da fun√ß√£o.

![Evid√™ncia 10](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia10.png)

Ap√≥s executar ap√≥s esse ajuste a fun√ß√£o retornou a resposta indicando que o arquivo foi processado, mostrando o n√∫mero de linhas lidas.

![Evid√™ncia 11](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia11.png)



## 2.4 Lab AWS - Limpeza de recursos

Ap√≥s concluir todas as etapas do laborat√≥rio na AWS e salvar todos os c√≥digos e evid√™ncias no meu GitHub, realizei a limpeza completa dos recursos utilizados para evitar cobran√ßas desnecess√°rias. Comecei excluindo todos os arquivos que foram usados ou gerados durante o processo no bucket S3, como o arquivo nomes.csv e o minha-camada-pandas.zip.

Em seguida, removi a camada personalizada que criei na AWS Lambda, bem como a fun√ß√£o Lambda utilizada no projeto. Tamb√©m verifiquei se n√£o havia  arquivos residuais, assim, finalizei o laborat√≥rio com todos os recursos devidamente limpos, mantendo apenas os registros e scripts relevantes salvos localmente e no reposit√≥rio.

![Evid√™ncia 12](./Exerc√≠cios/Exerc√≠cio3/Evid√™ncias/Evidencia12.png)


---

<br>

# Certificados


Durante essa sprint, conclu√≠ os cursos AWS Partner: Sales Accreditation (Business) (Portugu√™s), Cloud Economics (Portugu√™s) e Cloud Quest Practitioner. A seguir, compartilho os certificados correspondentes:

| Certificado | Link |
|--------|------|
|AWS Partner Sales Accreditation | [üîó Sales Accreditation](./Certificados/AWS%20Partner%20Sales%20Accreditation.pdf) |
|AWS Partner Cloud Economics Course | [üîó Cloud Economics](./Certificados/AWS%20Partner%20Cloud%20Economics%20Course%20Assessment.pdf) |
|AWS Cloud Quest Practitioner | [üîó Cloud Quest](https://www.credly.com/badges/e6de8a04-d292-415f-a306-bca96da7177f/public_url) |